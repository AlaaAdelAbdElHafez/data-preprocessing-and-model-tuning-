{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "üåà **Fully-connected model for sequential data**: Fully-connected models are not typically well-suited for sequential data because they don't inherently capture the temporal dependencies present in such data. For sequential data, recurrent neural networks (RNNs) or their variants, like LSTMs or GRUs, are more appropriate due to their ability to process sequences.\n",
        "\n",
        "üåà **Fully-connected model for image data**: Fully-connected models can be effective for image data, especially in tasks where spatial information is less important or has been flattened into a vector representation. However, for tasks like image classification or object detection where spatial relationships are crucial, convolutional neural networks (CNNs) are generally preferred because they can capture local patterns and spatial hierarchies effectively.\n",
        "\n",
        "üåà **Gradient vanishing and explosion**: Gradient vanishing and explosion refer to the phenomena where gradients in deep neural networks become very small (vanishing) or very large (explosion), making training difficult. GRU (Gated Recurrent Unit) and LSTM (Long Short-Term Memory) are types of RNNs designed to mitigate these issues by using mechanisms such as gating units and memory cells to control the flow of gradients and information over long sequences.\n",
        "\n",
        "üåà **Multi-objective/multi-task learning**: Multi-objective or multi-task learning involves training a single model to optimize multiple objectives or perform multiple tasks simultaneously. This can lead to better generalization and efficiency by allowing the model to learn shared representations across tasks. Multi-modality learning involves learning from multiple modalities of data (e.g., text, images, audio) simultaneously. In this assignment, you might use multi-task learning to jointly train a model for multiple related tasks, and multi-modality learning if your data includes diverse types of input.\n",
        "\n",
        "üåà **Difference among xgboost, lightgbm, and catboost**: XGBoost, LightGBM, and CatBoost are all gradient boosting algorithms commonly used for supervised learning tasks. They differ primarily in their implementation details and performance optimizations. LightGBM and CatBoost are both designed to be faster and more memory-efficient than XGBoost, with CatBoost specifically focusing on handling categorical features more effectively. Each algorithm may perform differently depending on the dataset and specific use case, so it's often beneficial to experiment with all three to determine which works best for a particular problem.\n",
        "\n",
        "\n",
        "\n",
        "**Problem Definition:**\n",
        "The problem is to predict the pricing category (beginner, plus, premium) for new Airbnb listings based on listing characteristics. Instead of providing a specific price, the model will recommend a pricing range to new hosts.\n",
        "\n",
        "**Input:**\n",
        "The input consists of various characteristics of the Airbnb listings in Montreal during 2019. These characteristics could include features such as location, property type, number of bedrooms, amenities, image, etc.\n",
        "\n",
        "**Output:**\n",
        "The output is the predicted pricing category for each listing: beginner (0), plus (1), or premium (2) and the type.\n",
        "\n",
        "**Data Mining Function:**\n",
        "Classification is required to predict the pricing category, type of the listings. Since the problem involves multi-objective and multi-modality learning, a multi-task learning approach might be employed to jointly optimize the model for multiple related tasks.\n",
        "\n",
        "**Challenges:**\n",
        "- Handling diverse listing characteristics and extracting relevant features.\n",
        "- Dealing with categorical variables and encoding them appropriately for the model.\n",
        "- Ensuring the model generalizes well to new listings and different areas within Montreal.\n",
        "\n",
        "**Impact:**\n",
        "An accurate pricing recommendation system can enhance user experience for new hosts on Airbnb by providing them with guidance on pricing their listings. This can potentially increase the number of hosts and listings on the platform, benefiting both hosts and guests.\n",
        "\n",
        "**Ideal Solution:**\n",
        "An ideal solution would involve a robust machine learning model trained on the dataset of Montreal Airbnb listings, utilizing multi-task learning to simultaneously predict pricing categories and potentially other related tasks. The model should be capable of handling various listing characteristics and providing accurate pricing recommendations for new hosts. Additionally, the solution should be scalable and adaptable to accommodate changes in the Airbnb marketplace over time."
      ],
      "metadata": {
        "id": "pqLJM0RTyYDZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preparations"
      ],
      "metadata": {
        "id": "KUIBnkqu_Fwj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "KiMrkOjeWj1A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a193fc90-466a-4b81-f3df-928f9bd7bd08"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-04-29 17:32:20--  https://github.com/CISC-873/Information-2021/releases/download/data/a4-5.zip\n",
            "Resolving github.com (github.com)... 140.82.113.4\n",
            "Connecting to github.com (github.com)|140.82.113.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/406495726/4d095bba-8b9b-4be4-8738-83f8ff5b0d18?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAVCODYLSA53PQK4ZA%2F20240429%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240429T173221Z&X-Amz-Expires=300&X-Amz-Signature=60453a6cb314ab0f23df24bc14fdd9c853599006e5c317db3d0122ace46b1390&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=406495726&response-content-disposition=attachment%3B%20filename%3Da4-5.zip&response-content-type=application%2Foctet-stream [following]\n",
            "--2024-04-29 17:32:21--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/406495726/4d095bba-8b9b-4be4-8738-83f8ff5b0d18?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAVCODYLSA53PQK4ZA%2F20240429%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240429T173221Z&X-Amz-Expires=300&X-Amz-Signature=60453a6cb314ab0f23df24bc14fdd9c853599006e5c317db3d0122ace46b1390&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=406495726&response-content-disposition=attachment%3B%20filename%3Da4-5.zip&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 639078419 (609M) [application/octet-stream]\n",
            "Saving to: ‚Äòa4-5.zip‚Äô\n",
            "\n",
            "a4-5.zip            100%[===================>] 609.47M  70.5MB/s    in 7.9s    \n",
            "\n",
            "2024-04-29 17:32:28 (77.6 MB/s) - ‚Äòa4-5.zip‚Äô saved [639078419/639078419]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# First let's download and unzip the dataset\n",
        "\n",
        "! wget https://github.com/CISC-873/Information-2021/releases/download/data/a4-5.zip\n",
        "! unzip -q a4-5.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "zQuimpjMjedi",
        "outputId": "e6621c7a-1252-4fff-f50c-8533c1afac4d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                summary               image  \\\n",
              "0     Spacious, sunny and cozy modern apartment in t...     img_train/0.jpg   \n",
              "1     Located in one of the most vibrant and accessi...     img_train/1.jpg   \n",
              "2     Logement coquet et douillet √† 10 minutes du ce...     img_train/2.jpg   \n",
              "3     Beautiful and spacious (1076 sc ft, / 100 mc) ...     img_train/3.jpg   \n",
              "4     Tr√®s grand appartement ''rustique'' et tr√®s ag...     img_train/4.jpg   \n",
              "...                                                 ...                 ...   \n",
              "7622  Un grand logement 4 et 1/2, tout inclut, bien ...  img_train/7626.jpg   \n",
              "7623  Magnificent condo directly on the river. You w...  img_train/7627.jpg   \n",
              "7624  This apartment is perfect for anyone visiting ...  img_train/7628.jpg   \n",
              "7625  It is a cozy ,clean ,and comfortable apartment...  img_train/7629.jpg   \n",
              "7626  Modern country style (newly-renovated); open c...  img_train/7630.jpg   \n",
              "\n",
              "           type  price  \n",
              "0     Apartment      1  \n",
              "1     Apartment      0  \n",
              "2     Apartment      1  \n",
              "3     Apartment      1  \n",
              "4     Apartment      0  \n",
              "...         ...    ...  \n",
              "7622  Apartment      0  \n",
              "7623  Apartment      2  \n",
              "7624  Apartment      1  \n",
              "7625  Apartment      0  \n",
              "7626      House      1  \n",
              "\n",
              "[7627 rows x 4 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ba476a30-d390-4667-ad7a-538181ae7cbd\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>summary</th>\n",
              "      <th>image</th>\n",
              "      <th>type</th>\n",
              "      <th>price</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Spacious, sunny and cozy modern apartment in t...</td>\n",
              "      <td>img_train/0.jpg</td>\n",
              "      <td>Apartment</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Located in one of the most vibrant and accessi...</td>\n",
              "      <td>img_train/1.jpg</td>\n",
              "      <td>Apartment</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Logement coquet et douillet √† 10 minutes du ce...</td>\n",
              "      <td>img_train/2.jpg</td>\n",
              "      <td>Apartment</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Beautiful and spacious (1076 sc ft, / 100 mc) ...</td>\n",
              "      <td>img_train/3.jpg</td>\n",
              "      <td>Apartment</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Tr√®s grand appartement ''rustique'' et tr√®s ag...</td>\n",
              "      <td>img_train/4.jpg</td>\n",
              "      <td>Apartment</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7622</th>\n",
              "      <td>Un grand logement 4 et 1/2, tout inclut, bien ...</td>\n",
              "      <td>img_train/7626.jpg</td>\n",
              "      <td>Apartment</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7623</th>\n",
              "      <td>Magnificent condo directly on the river. You w...</td>\n",
              "      <td>img_train/7627.jpg</td>\n",
              "      <td>Apartment</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7624</th>\n",
              "      <td>This apartment is perfect for anyone visiting ...</td>\n",
              "      <td>img_train/7628.jpg</td>\n",
              "      <td>Apartment</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7625</th>\n",
              "      <td>It is a cozy ,clean ,and comfortable apartment...</td>\n",
              "      <td>img_train/7629.jpg</td>\n",
              "      <td>Apartment</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7626</th>\n",
              "      <td>Modern country style (newly-renovated); open c...</td>\n",
              "      <td>img_train/7630.jpg</td>\n",
              "      <td>House</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>7627 rows √ó 4 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ba476a30-d390-4667-ad7a-538181ae7cbd')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-ba476a30-d390-4667-ad7a-538181ae7cbd button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-ba476a30-d390-4667-ad7a-538181ae7cbd');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-5a3cdf8f-ce7c-40cc-bdd9-e074094651ec\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-5a3cdf8f-ce7c-40cc-bdd9-e074094651ec')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-5a3cdf8f-ce7c-40cc-bdd9-e074094651ec button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_b83d1a5a-106c-4943-9137-51bc00e64d16\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_b83d1a5a-106c-4943-9137-51bc00e64d16 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 7627,\n  \"fields\": [\n    {\n      \"column\": \"summary\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 6844,\n        \"samples\": [\n          \"Le \\u00ab Cabin \\u00bb- est un loft de style rustique et artistique. Mon appartement est un nid chaleureux unique en son genre, bien \\u00e9quip\\u00e9 pour bien relaxer et \\u00eatre tr\\u00e8s efficace.  Enti\\u00e8rement meubl\\u00e9 et conviviale. Terrasse priv\\u00e9e intime avec SPA, barbecue et abris pluie. Stationnement priv\\u00e9, facilement accessible en voiture, \\u00e0 10 mins du centre-ville. En plein c\\u0153ur du quartier HoMa, tout \\u00e0 distance de marche!\",\n          \"Beautiful two floor apartment in St-Henri fully equipped! Our home is a 7 mins walk from the Place St Henri metro station, 5 min from the Atwater market, 2 mins from the Lachine canal bike path, and near a wide variety of restaurants on Notre Dame street. We believe the area and our home is great base to explore the city by foot, bike (nearby Bixi station) or car (street parking). The entire apartment will be available to guests during this time as we will be on vacation.\",\n          \"Grand 5 1/2 (2 chambres) \\u00e0 aire ouverte , vert de plantes et lumineux . Id\\u00e9al pour d\\u00e9couvrir Montr\\u00e9al: -\\u00e0 1 minute \\u00e0 pied du m\\u00e9tro Jean-Talon -\\u00e0 7 minutes \\u00e0 pied du march\\u00e9 Jean-Talon  -\\u00e0 10 minutes \\u00e0 pied de la Petite-Italie -\\u00e0 12 minutes \\u00e0 pied du parc Jarry  - de nombreux commerces de proximit\\u00e9 ( restaurants, bars, caf\\u00e9s, cr\\u00e8meries...)\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"image\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 7627,\n        \"samples\": [\n          \"img_train/683.jpg\",\n          \"img_train/1964.jpg\",\n          \"img_train/4151.jpg\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"type\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 24,\n        \"samples\": [\n          \"Casa particular (Cuba)\",\n          \"Other\",\n          \"Apartment\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"price\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 2,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          1,\n          0,\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "import pandas as pd\n",
        "from tqdm.notebook import tqdm\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "from ast import literal_eval\n",
        "\n",
        "# after unzipping the datset\n",
        "# we find that (through the file browser on the left) there is a csv file and a\n",
        "# folder of images\n",
        "df = pd.read_csv('train_xy.csv')\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's encode the prediction labels and calculate the total number\n",
        "of unique labels. After, lets split the dataset into training set and testing set.\n",
        "The 'type' column of the DataFrame 'df' is converted to a categorical type using the astype method and then encoded as numerical codes using the cat.codes method. This is typically done to prepare categorical data for machine learning models.\n",
        "The variable len_type is assigned the number of unique categories in the 'type' column of the DataFrame 'df'. This could be useful for understanding the distribution of categories in the dataset.\n",
        "Similarly, the variable len_price is assigned the number of unique values in the 'price' column of the DataFrame 'df'. This could be useful for understanding the range and distribution of prices in the dataset.\n",
        "The data from the CSV file 'test_x.csv' is read into a new DataFrame named 'dftest'. This dataset contains test data that will be used to evaluate the model's performance.\n",
        "The contents of the DataFrame 'dftest' are displayed. This is done to inspect the structure and contents of the test data."
      ],
      "metadata": {
        "id": "_fYGsgAi_eIY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "1z3FEfYsR1Zm",
        "outputId": "a2d8dfe9-41cb-4a5b-d48e-897351a3e55b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        id                                            summary  \\\n",
              "0        0  Charming warm house is ready to host you here ...   \n",
              "1        1  La chambre est spacieuse et lumineuse, dans un...   \n",
              "2        2  Grande chambre confortable situ√©e au sous-sol ...   \n",
              "3        3  Pr√®s d‚Äôun M√©tro, ligne orange. 10 minutes √† pi...   \n",
              "4        4  Very bright appartment and very cosy. 2 separa...   \n",
              "...    ...                                                ...   \n",
              "7355  7626  Large, fully-furnished flat with brick walls a...   \n",
              "7356  7627  Logement situ√© dans le haut d‚Äôun duplex. Vivez...   \n",
              "7357  7628  My place is close to parks, . My place is good...   \n",
              "7358  7629  *** For security reasons, I will prioritize gu...   \n",
              "7359  7630  Stay in an amazing area of Montreal! 5-7 min f...   \n",
              "\n",
              "                  image  \n",
              "0        img_test/0.jpg  \n",
              "1        img_test/1.jpg  \n",
              "2        img_test/2.jpg  \n",
              "3        img_test/3.jpg  \n",
              "4        img_test/4.jpg  \n",
              "...                 ...  \n",
              "7355  img_test/7627.jpg  \n",
              "7356  img_test/7628.jpg  \n",
              "7357  img_test/7629.jpg  \n",
              "7358  img_test/7630.jpg  \n",
              "7359  img_test/7631.jpg  \n",
              "\n",
              "[7360 rows x 3 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d3a36e73-ddb8-4e68-892f-09a28850636b\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>summary</th>\n",
              "      <th>image</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>Charming warm house is ready to host you here ...</td>\n",
              "      <td>img_test/0.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>La chambre est spacieuse et lumineuse, dans un...</td>\n",
              "      <td>img_test/1.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>Grande chambre confortable situ√©e au sous-sol ...</td>\n",
              "      <td>img_test/2.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>Pr√®s d‚Äôun M√©tro, ligne orange. 10 minutes √† pi...</td>\n",
              "      <td>img_test/3.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>Very bright appartment and very cosy. 2 separa...</td>\n",
              "      <td>img_test/4.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7355</th>\n",
              "      <td>7626</td>\n",
              "      <td>Large, fully-furnished flat with brick walls a...</td>\n",
              "      <td>img_test/7627.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7356</th>\n",
              "      <td>7627</td>\n",
              "      <td>Logement situ√© dans le haut d‚Äôun duplex. Vivez...</td>\n",
              "      <td>img_test/7628.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7357</th>\n",
              "      <td>7628</td>\n",
              "      <td>My place is close to parks, . My place is good...</td>\n",
              "      <td>img_test/7629.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7358</th>\n",
              "      <td>7629</td>\n",
              "      <td>*** For security reasons, I will prioritize gu...</td>\n",
              "      <td>img_test/7630.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7359</th>\n",
              "      <td>7630</td>\n",
              "      <td>Stay in an amazing area of Montreal! 5-7 min f...</td>\n",
              "      <td>img_test/7631.jpg</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>7360 rows √ó 3 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d3a36e73-ddb8-4e68-892f-09a28850636b')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-d3a36e73-ddb8-4e68-892f-09a28850636b button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-d3a36e73-ddb8-4e68-892f-09a28850636b');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-46f7a9cd-b80f-48e0-b7fe-fc83442381bf\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-46f7a9cd-b80f-48e0-b7fe-fc83442381bf')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-46f7a9cd-b80f-48e0-b7fe-fc83442381bf button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_ee7da009-2126-482b-9327-04baba6259dc\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('dftest')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_ee7da009-2126-482b-9327-04baba6259dc button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('dftest');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "dftest",
              "summary": "{\n  \"name\": \"dftest\",\n  \"rows\": 7360,\n  \"fields\": [\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2202,\n        \"min\": 0,\n        \"max\": 7630,\n        \"num_unique_values\": 7360,\n        \"samples\": [\n          1969,\n          4547,\n          7074\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"summary\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 6903,\n        \"samples\": [\n          \"Welcome to this  BRAND NEW very central unit located in the heart of Montreal. EXPECT ONE OF THE BEST LOCATION EVER, 15 minutes ride to Downtown, GARAGE PARKING ,close to parcs, cycling path, public transportation and only 20 minutes away from Montreal Trudeau airport. 2 complete bedrooms with air-conditioning, equipped kitchen, and a wonderful place to explore Montreal. We offer all kind of recommendations.  Subway station is very near, Tennis court, swimming pool, etc\",\n          \"Located in the heart of MTL, between downtown & Plateau Mont-Royal, this cozy place is close to everything ( metro (subway), bus, restos, bars, groceries, shows, shops, old port)! Amazing rooftop view of MTL. Perfect for single/couple travellers.\",\n          \"Vivez votre exp\\u00e9rience montr\\u00e9alaise en toute \\u00e9l\\u00e9gance dans cet appartement moderne et tout \\u00e9quip\\u00e9 situ\\u00e9 aux portes du Vieux-Port de Montr\\u00e9al. Situer \\u00e0 quelque pas du m\\u00e9tro Place d'armes et de Chinatown. Dispose d'une cuisine d'inspiration designer avec trois nouveaux appareils \\u00e9lectrom\\u00e9nagers en acier (Website hidden by Airbnb) vous n\\u2019avez pas forc\\u00e9ment envie de sortir,  une grande salle commune sont \\u00e0 votre disposition. Il y a aussi un bar et une terrasse au niveau lobby.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"image\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 7360,\n        \"samples\": [\n          \"img_test/1969.jpg\",\n          \"img_test/4547.jpg\",\n          \"img_test/7075.jpg\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "\n",
        "\n",
        "# Convert the 'type' column of the DataFrame 'df' to categorical type and then encode it as numerical codes\n",
        "df['type'] = df.type.astype('category').cat.codes\n",
        "\n",
        "# Calculate the number of unique categories in the 'type' column of the DataFrame 'df'\n",
        "len_type = len(df.type.unique())\n",
        "\n",
        "# Calculate the number of unique values in the 'price' column of the DataFrame 'df'\n",
        "len_price = len(df.price.unique())\n",
        "\n",
        "# Read the test data from the CSV file 'test_x.csv' into a DataFrame named 'dftest'\n",
        "dftest = pd.read_csv('test_x.csv')\n",
        "\n",
        "# Display the contents of the DataFrame 'dftest'\n",
        "dftest\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preprocessing\n",
        "\n",
        "We have image and text data.\n",
        "\n",
        "- Image data: resize\n",
        "- Text data: tokenization and converting to integer IDs"
      ],
      "metadata": {
        "id": "FUoMACSf_MRK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code defines a function named load_image that loads and preprocesses image files. It attempts to open the image file specified by the input path, converts it to grayscale, resizes it to 64x64 pixels, and converts it to a NumPy array. If an error occurs during loading or preprocessing (e.g., if the file is missing or corrupted), it returns a zero-filled array.\n",
        "Images from the 'image' column of the DataFrame 'df' are loaded and preprocessed using the load_image function. The resulting array of image data is stored in the variable x_train_image.\n",
        "Text data from the 'summary' column of the DataFrame 'df' is loaded into the variable x_train_text. Non-string cells are forced to be converted to strings using the astype('str') method.\n",
        "The 'type' column of the DataFrame 'df' is extracted and stored in the variable y_train_type. This column represents the target variable for classification.\n",
        "Similarly, the 'price' column of the DataFrame 'df' is extracted and stored in the variable y_train_price. This column represents the target variable for regression."
      ],
      "metadata": {
        "id": "BLoQp1Gu92l_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "145c5789145446628272efe0505f35cd",
            "01b03fbd9d1b4f889df21be28dfe5670",
            "28d8043ae0684e45841c78b9fdce2a10",
            "09446fec7b4f4695bda5d71d2a8c1ce8",
            "7f0e5df515644b0d989b78af52cc1a5d",
            "6c65ebb44970417a96605d8f5b5d11ca",
            "ffe6795d2f6e401b90a0e75d54396fd7",
            "6947f075224645e4a67dfaa90a9e0e5d",
            "9f37c9f0b5e84c6c84c25a049388cf8d",
            "54faf5df85c642ff9404d62f5e760d43",
            "e85897b112c34b339b2627bfa92516a6"
          ]
        },
        "id": "7NGojpH_R1Zl",
        "outputId": "6723305b-1f0c-4d5f-ef0d-e56d414a4fc6"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/7627 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "145c5789145446628272efe0505f35cd"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Importing the os module, which provides a portable way of using operating system dependent functionality\n",
        "import os\n",
        "\n",
        "# Define a function named load_image that takes a file path as input and returns the image data as a NumPy array\n",
        "def load_image(file):\n",
        "    try:\n",
        "        # Attempt to open and preprocess the image file: convert to grayscale, resize to 64x64 pixels\n",
        "        image = Image.open(file).convert('LA').resize((64, 64))\n",
        "        # Convert the image to a NumPy array\n",
        "        arr = np.array(image)\n",
        "    except:\n",
        "        # If an error occurs during image loading or preprocessing, return a zero-filled array\n",
        "        arr = np.zeros((64, 64, 2))\n",
        "    return arr\n",
        "\n",
        "# Preprocess image data:\n",
        "# Load images from the 'image' column of the DataFrame 'df' and resize them to 64x64 pixels\n",
        "x_train_image = np.array([load_image(i) for i in tqdm(df.image)])\n",
        "\n",
        "# Load text data from the 'summary' column of the DataFrame 'df' and convert non-string cells to strings\n",
        "#x_train_text = df.summary.astype('str')\n",
        "x_train_text = pd.DataFrame(df['summary'].astype(str), columns=['summary'])\n",
        "\n",
        "\n",
        "# Extract the 'type' column from the DataFrame 'df' as the target variable for classification\n",
        "y_train_type = df.type\n",
        "\n",
        "# Extract the 'price' column from the DataFrame 'df' as the target variable for regression\n",
        "y_train_price = df.price\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code imports the pyplot module from the matplotlib library, which provides functions for creating plots and visualizations.\n",
        "It uses the imshow() function from pyplot to display the first image from the preprocessed image data stored in the variable x_train_image.\n",
        "The image is displayed in grayscale, as indicated by the indexing [:, :, 0], which selects the first channel (grayscale) of the image array."
      ],
      "metadata": {
        "id": "vAFxXUWY-NJB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "Ayqfe7rQS0Mk",
        "outputId": "6712cfd9-e8bc-4176-f9a4-321522b4abdc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7b3dcbbdb580>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Import the pyplot module from matplotlib, which provides a MATLAB-like plotting framework\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Display the first image from the preprocessed image data (grayscale channel)\n",
        "plt.imshow(x_train_image[0, :, :, 0])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the googletrans library version 4.0.0-rc1 using pip\n",
        "!pip install googletrans==4.0.0-rc1\n",
        "!pip install googletrans"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nR32BwJOR35G",
        "outputId": "f9e3fb51-a5e8-4eb9-a881-bb5398f89d51"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting googletrans==4.0.0-rc1\n",
            "  Downloading googletrans-4.0.0rc1.tar.gz (20 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting httpx==0.13.3 (from googletrans==4.0.0-rc1)\n",
            "  Downloading httpx-0.13.3-py3-none-any.whl (55 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m55.1/55.1 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2024.2.2)\n",
            "Collecting hstspreload (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading hstspreload-2024.4.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (1.3.1)\n",
            "Collecting chardet==3.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m133.4/133.4 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting idna==2.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading idna-2.10-py2.py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rfc3986<2,>=1.3 (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n",
            "Collecting httpcore==0.9.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading httpcore-0.9.1-py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.10,>=0.8 (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading h11-0.9.0-py2.py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h2==3.* (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading h2-3.2.0-py2.py3-none-any.whl (65 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m65.0/65.0 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting hyperframe<6,>=5.2.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading hyperframe-5.2.0-py2.py3-none-any.whl (12 kB)\n",
            "Collecting hpack<4,>=3.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading hpack-3.0.0-py2.py3-none-any.whl (38 kB)\n",
            "Building wheels for collected packages: googletrans\n",
            "  Building wheel for googletrans (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for googletrans: filename=googletrans-4.0.0rc1-py3-none-any.whl size=17395 sha256=df9f091c4c10f37ae32335b0c13e86a5042505a0ffa080f33e5b33a47f57332f\n",
            "  Stored in directory: /root/.cache/pip/wheels/c0/59/9f/7372f0cf70160fe61b528532e1a7c8498c4becd6bcffb022de\n",
            "Successfully built googletrans\n",
            "Installing collected packages: rfc3986, hyperframe, hpack, h11, chardet, idna, hstspreload, h2, httpcore, httpx, googletrans\n",
            "  Attempting uninstall: chardet\n",
            "    Found existing installation: chardet 5.2.0\n",
            "    Uninstalling chardet-5.2.0:\n",
            "      Successfully uninstalled chardet-5.2.0\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.7\n",
            "    Uninstalling idna-3.7:\n",
            "      Successfully uninstalled idna-3.7\n",
            "Successfully installed chardet-3.0.4 googletrans-4.0.0rc1 h11-0.9.0 h2-3.2.0 hpack-3.0.0 hstspreload-2024.4.1 httpcore-0.9.1 httpx-0.13.3 hyperframe-5.2.0 idna-2.10 rfc3986-1.5.0\n",
            "Requirement already satisfied: googletrans in /usr/local/lib/python3.10/dist-packages (4.0.0rc1)\n",
            "Requirement already satisfied: httpx==0.13.3 in /usr/local/lib/python3.10/dist-packages (from googletrans) (0.13.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans) (2024.2.2)\n",
            "Requirement already satisfied: hstspreload in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans) (2024.4.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans) (1.3.1)\n",
            "Requirement already satisfied: chardet==3.* in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans) (3.0.4)\n",
            "Requirement already satisfied: idna==2.* in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans) (2.10)\n",
            "Requirement already satisfied: rfc3986<2,>=1.3 in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans) (1.5.0)\n",
            "Requirement already satisfied: httpcore==0.9.* in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans) (0.9.1)\n",
            "Requirement already satisfied: h11<0.10,>=0.8 in /usr/local/lib/python3.10/dist-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans) (0.9.0)\n",
            "Requirement already satisfied: h2==3.* in /usr/local/lib/python3.10/dist-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans) (3.2.0)\n",
            "Requirement already satisfied: hyperframe<6,>=5.2.0 in /usr/local/lib/python3.10/dist-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans) (5.2.0)\n",
            "Requirement already satisfied: hpack<4,>=3.0 in /usr/local/lib/python3.10/dist-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans) (3.0.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code imports the Translator class from the googletrans module for text translation.\n",
        "A function named translate_text is defined to translate text to English. Inside this function, the language of the input text is detected using the detect method of the Translator object.\n",
        "If the detected language is already English, a message is printed and the original text is returned. Otherwise, the text is translated to English using the translate method of the Translator object.\n",
        "Inside a loop, each text in x_train_text is translated using the translate_text function, and the translated texts are stored in a list named translated_texts.\n",
        "After all texts have been translated, the translated_texts list is assigned to x_train_text."
      ],
      "metadata": {
        "id": "lll1hIgl-hCr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U deep-translator\n",
        "!poetry add deep-translator   # for poetry usage\n",
        "\n",
        "!pip install deepl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZKULTNfXuMcF",
        "outputId": "d5d352d2-6cb5-4938-8168-fd88cfc0c1ca"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting deep-translator\n",
            "  Downloading deep_translator-1.11.4-py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m42.3/42.3 kB\u001b[0m \u001b[31m422.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: beautifulsoup4<5.0.0,>=4.9.1 in /usr/local/lib/python3.10/dist-packages (from deep-translator) (4.12.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from deep-translator) (2.31.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep-translator) (2.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2024.2.2)\n",
            "Installing collected packages: deep-translator\n",
            "Successfully installed deep-translator-1.11.4\n",
            "/bin/bash: line 1: poetry: command not found\n",
            "Collecting deepl\n",
            "  Downloading deepl-1.18.0-py3-none-any.whl (35 kB)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from deepl) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->deepl) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->deepl) (2.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->deepl) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->deepl) (2024.2.2)\n",
            "Installing collected packages: deepl\n",
            "Successfully installed deepl-1.18.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from google.cloud import translate_v2 as translate\n",
        "import time\n",
        "from googletrans import Translator\n",
        "def translate_to_english(text):\n",
        "    # Initialize Google Cloud Translation API client\n",
        "    translate_client = Translator()\n",
        "    translate_client.raise_Exception = True\n",
        "    try:\n",
        "      translated_text = translate_client.translate(text, dest='en')\n",
        "      return translated_text\n",
        "        # Detect language\n",
        "        #detected_language = translate_client.detect_language(text)['language']\n",
        "\n",
        "        #else:\n",
        "         #   return text  # Return original text if already in English\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Translation error: {e}\")\n",
        "        # Handle too many requests\n",
        "        if '429' in str(e):\n",
        "            translate_client = Translator()\n",
        "            translate_client.raise_Exception = True\n",
        "            translated_text = translate_client.translate(text, dest='en')\n",
        "            print(\"Unexpected error, skipping translation...\")\n",
        "            return translated_text  # Return original text in case of error\n",
        "\n",
        "# Convert the \"summary\" column of DataFrame to a DataFrame with one column\n",
        "#x_train_text = pd.DataFrame(df['summary'].astype(str), columns=['x_train_text'])\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JemN86QpW_Cs"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code imports various libraries and modules for data manipulation, natural language processing (NLP), and machine learning. Regular expressions are compiled to facilitate text cleaning tasks, including matching URLs, HTML tags, non-alphabetic characters, and single characters. Functions and resources for text preprocessing and cleaning are initialized, including a SnowballStemmer for word stemming, a set of English stopwords, and a WordNetLemmatizer for word lemmatization. The cleantext function is defined to clean text data by substituting patterns with specified replacements, tokenizing text into words, converting tokens to lowercase, and applying stemming, lemmatization, and stop words removal based on the value of the embedding parameter."
      ],
      "metadata": {
        "id": "sbu7XYe5prPh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re  # Importing the regular expression module for text cleaning\n",
        "import pickle  # Importing the pickle module for saving and loading Python objects\n",
        "import sklearn  # Importing the scikit-learn library for machine learning algorithms\n",
        "import pandas as pd  # Importing pandas library for data manipulation and analysis\n",
        "import numpy as np  # Importing numpy library for numerical computations\n",
        "import holoviews as hv  # Importing holoviews library for visualization\n",
        "import nltk  # Importing nltk library for natural language processing tasks\n",
        "from nltk.stem.snowball import SnowballStemmer  # Importing SnowballStemmer for word stemming\n",
        "from nltk.stem import WordNetLemmatizer  # Importing WordNetLemmatizer for word lemmatization\n",
        "from nltk.tokenize import word_tokenize  # Importing word_tokenize for tokenization\n",
        "from nltk.corpus import stopwords  # Importing stopwords from nltk corpus\n",
        "nltk.download('punkt')  # Downloading the punkt tokenizer models\n",
        "nltk.download('stopwords')  # Downloading the stopwords corpus\n",
        "nltk.download(\"all\")  # Downloading all nltk data (not typically recommended due to large size)\n",
        "\n",
        "# Initializing SnowballStemmer for English language\n",
        "stemmer = SnowballStemmer(\"english\")\n",
        "\n",
        "# Creating a set of English stopwords for text cleaning\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "# Initializing WordNetLemmatizer for word lemmatization\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def cleantext(text, embedding=False):\n",
        "    # Compiling regular expressions for cleaning text\n",
        "    links = re.compile(r'https?://\\S+', re.IGNORECASE)  # Regular expression to match URLs\n",
        "    Wspace = re.compile(r\"\\s+\", re.IGNORECASE)  # Regular expression to match white spaces\n",
        "    tags = re.compile(r\"<[^>]+>\")  # Regular expression to match HTML tags\n",
        "    ASCII = re.compile(r\"[^A-Za-z ]\", re.IGNORECASE)  # Regular expression to match non-alphabetic characters\n",
        "    singlechar = re.compile(r\"\\b[A-Za-z]\\b\", re.IGNORECASE)  # Regular expression to match single characters\n",
        "\n",
        "    # Handling punctuation and stop words if embedding is True\n",
        "    if embedding:\n",
        "        # Keep punctuation by modifying ASCII regex\n",
        "        ASCII = re.compile(r\"[^A-Za-z,.!? ]\", re.IGNORECASE)\n",
        "        # Keep single characters by modifying singlechar regex\n",
        "        singlechar = re.compile(r\"\\b[A-Za-z,.!?]\\b\", re.IGNORECASE)\n",
        "\n",
        "    # Cleaning text by substituting patterns with specified replacements\n",
        "    text = re.sub(links, \"link\", text)  # Substituting URLs with \"link\"\n",
        "    text = re.sub(tags, \" \", text)  # Removing HTML tags\n",
        "    text = re.sub(ASCII, \" \", text)  # Removing non-alphabetic characters\n",
        "    text = re.sub(singlechar, \" \", text)  # Removing single characters\n",
        "    text = re.sub(Wspace, \" \", text)  # Removing extra white spaces\n",
        "\n",
        "    # Tokenizing text into words\n",
        "    tokens = word_tokenize(text)\n",
        "    # Converting tokens to lowercase\n",
        "    tokens_lower = [word.lower() for word in tokens]\n",
        "\n",
        "    # Handling stemming, lemmatization, and stop words removal based on the value of embedding\n",
        "    if embedding:\n",
        "        # No stemming, lowering, and punctuation / stop words removal\n",
        "        words_filtered = tokens\n",
        "    else:\n",
        "        # Applying lemmatization and stop words removal\n",
        "        words_filtered = [lemmatizer.lemmatize(word) for word in tokens_lower if word not in stop_words]\n",
        "\n",
        "    # Joining filtered words back into a clean text string\n",
        "    text_clean = \" \".join(words_filtered)\n",
        "    return text_clean  # Returning the cleaned text\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WacAt0mypkYH",
        "outputId": "482d972e-513e-42d1-8d8c-112ef8a6a52c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n",
            "[nltk_data]    | Downloading package bcp47 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dolch.zip.\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n",
            "[nltk_data]    | Downloading package extended_omw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/floresta.zip.\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/indian.zip.\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/kimmo.zip.\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/moses_sample.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/paradigms.zip.\n",
            "[nltk_data]    | Downloading package pe08 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pe08.zip.\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/perluniprops.zip.\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pil.zip.\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pl196x.zip.\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ptb.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Package punkt is already up-to-date!\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/qc.zip.\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/rslp.zip.\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/rte.zip.\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/smultron.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Package stopwords is already up-to-date!\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/switchboard.zip.\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet.zip.\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet2022 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet2022.zip.\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ycoe.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(x_train_text['summary'][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dxq9y7B0vCSP",
        "outputId": "a80db48a-0682-4eb8-e468-d4ccc50a8b25"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "str"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    # Apply cleantext function to the 'summary' column\n",
        "    dftest['summary'] = dftest['summary'].astype(str).apply(cleantext)\n",
        "except TypeError as e:\n",
        "    print(\"Error:\", e)\n",
        "    print(\"Ensure the 'summary' column contains strings or bytes-like objects.\")\n"
      ],
      "metadata": {
        "id": "EGpwPRPNvfEX"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    # Apply cleantext function to the 'summary' column\n",
        "    x_train_text['summary'] = x_train_text['summary'].astype(str).apply(cleantext)\n",
        "except TypeError as e:\n",
        "    print(\"Error:\", e)\n",
        "    print(\"Ensure the 'summary' column contains strings or bytes-like objects.\")\n"
      ],
      "metadata": {
        "id": "WeZ-xnt_vmr1"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code imports required libraries for text preprocessing, including Tokenizer and pad_sequences from TensorFlow from scikit-learn.\n",
        "The vocabulary size and maximum sequence length are defined.\n",
        "A Tokenizer object is created and fitted on the training text data (x_train_text) to build the vocabulary.\n",
        "A function named _preprocess is defined to preprocess text data. This function converts text to sequences of integers using the fitted Tokenizer and pads sequences to a fixed length using pad_sequences.\n",
        "The preprocessed text data is obtained by calling the _preprocess function on the training text data (x_train_text).\n",
        "The shape of the preprocessed text data is printed to verify the preprocessing steps."
      ],
      "metadata": {
        "id": "PBgWY32K_oS5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Xh70MnCDR1Zm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb7a0302-d133-4b17-d08a-86839f7d659b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(7627, 100)\n"
          ]
        }
      ],
      "source": [
        "# Importing required libraries for text preprocessing\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from pprint import pprint\n",
        "\n",
        "# Define vocabulary size and maximum sequence length\n",
        "vocab_size = 40000\n",
        "max_len = 100\n",
        "\n",
        "# Build vocabulary from training set using Tokenizer\n",
        "tokenizer = Tokenizer(num_words=vocab_size)\n",
        "tokenizer.fit_on_texts(x_train_text)\n",
        "\n",
        "# Function to preprocess text data\n",
        "def _preprocess(list_of_text):\n",
        "    # Convert text to sequences of integers and pad sequences to a fixed length\n",
        "    return pad_sequences(\n",
        "        tokenizer.texts_to_sequences(list_of_text),\n",
        "        maxlen=max_len,\n",
        "        padding='post',\n",
        "    )\n",
        "\n",
        "# Preprocess text data: convert text to sequences of integers and pad sequences\n",
        "x_train_text_id = _preprocess(x_train_text[\"summary\"])\n",
        "# Print the shape of preprocessed text data\n",
        "print(x_train_text_id.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cZkf89o24mB-"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "p5E5zYSiR1Zm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "177b18c8-be5c-45fe-b3ba-7b7cf379cc79"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['', '', '', '', '']\n"
          ]
        }
      ],
      "source": [
        "# we can use the tokenizer to convert IDs to words.\n",
        "pprint(tokenizer.sequences_to_texts(x_train_text_id[:5]))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(x_train_text_id)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HqX9CWEzzbyZ",
        "outputId": "605d42f1-b784-4db6-f482-ce91d93a1e6e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "75tmwBWAR1Zm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78c6cd2d-ddeb-4a24-fb64-0e66ffe46220"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total words in the dictionary: 40000\n"
          ]
        }
      ],
      "source": [
        "print('total words in the dictionary:', tokenizer.num_words)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the distribution of the target variable\n",
        "target_distribution = y_train_price.value_counts()\n",
        "\n",
        "# Calculate the percentage of each class in the target variable\n",
        "class_percentages = target_distribution / target_distribution.sum() * 100\n",
        "\n",
        "# Display the result\n",
        "print(\"Class distribution:\")\n",
        "print(target_distribution)\n",
        "print(\"\\nClass percentages:\")\n",
        "print(class_percentages)\n",
        "\n",
        "# Visualize the distribution for better understanding\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "target_distribution.plot(kind='bar', color='skyblue')\n",
        "plt.title('Class Distribution')\n",
        "plt.xlabel('Class')\n",
        "plt.ylabel('Frequency')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 792
        },
        "id": "MrSk2luRwWdE",
        "outputId": "a2392943-1cdd-4b75-d6bf-cb07166d4f3d"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class distribution:\n",
            "price\n",
            "0    4737\n",
            "1    2403\n",
            "2     487\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Class percentages:\n",
            "price\n",
            "0    62.108299\n",
            "1    31.506490\n",
            "2     6.385210\n",
            "Name: count, dtype: float64\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# trial 1: Multi-modality learning. (text+image) model.Multi-objective learning.(predicting both price and type).\n",
        "It defines batch input shapes for text and image inputs using keras.Input.\n",
        "For the text part, an Embedding layer is applied to the input text, followed by average pooling to summarize the text information.\n",
        "For the image part, a Convolutional Neural Network (CNN) is applied to the input image, followed by max pooling and flattening to extract image features.\n",
        "The outputs from the text and image branches are concatenated to fuse the information from both modalities.\n",
        "The model employs multi-task learning with separate dense layers for predicting the type and price of the listing.\n",
        "Model inputs and outputs are defined using keys to specify the input and output names.\n",
        "The model is compiled with an optimizer, loss functions for each task, loss weights for each task, and evaluation metrics.\n",
        "Finally, the model summary is printed to display the architecture and parameters of the model."
      ],
      "metadata": {
        "id": "zJFWGD1eAKMD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "_QkkRmqYR1Zm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5e6734c-c68b-4655-973c-34689c2750dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_2 (InputLayer)        [(None, 64, 64, 2)]          0         []                            \n",
            "                                                                                                  \n",
            " input_1 (InputLayer)        [(None, 100)]                0         []                            \n",
            "                                                                                                  \n",
            " conv2d (Conv2D)             (None, 49, 49, 32)           16416     ['input_2[0][0]']             \n",
            "                                                                                                  \n",
            " embedding (Embedding)       (None, 100, 100)             4000000   ['input_1[0][0]']             \n",
            "                                                                                                  \n",
            " max_pooling2d (MaxPooling2  (None, 3, 3, 32)             0         ['conv2d[0][0]']              \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " tf.math.reduce_mean (TFOpL  (None, 100)                  0         ['embedding[0][0]']           \n",
            " ambda)                                                                                           \n",
            "                                                                                                  \n",
            " flatten (Flatten)           (None, 288)                  0         ['max_pooling2d[0][0]']       \n",
            "                                                                                                  \n",
            " tf.concat (TFOpLambda)      (None, 388)                  0         ['tf.math.reduce_mean[0][0]', \n",
            "                                                                     'flatten[0][0]']             \n",
            "                                                                                                  \n",
            " price (Dense)               (None, 3)                    1167      ['tf.concat[0][0]']           \n",
            "                                                                                                  \n",
            " type (Dense)                (None, 24)                   9336      ['tf.concat[0][0]']           \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 4026919 (15.36 MB)\n",
            "Trainable params: 4026919 (15.36 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Importing necessary modules and libraries\n",
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "import collections\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import Conv2D, MaxPool2D, Flatten, Dense, Dropout, LSTM, Bidirectional\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Define batch input shapes for text and image inputs\n",
        "in_text = keras.Input(batch_shape=(None, max_len))\n",
        "in_image = keras.Input(batch_shape=(None, 64, 64, 2))\n",
        "\n",
        "# Text part: Embedding layer followed by average pooling\n",
        "embedded = keras.layers.Embedding(tokenizer.num_words, 100)(in_text)\n",
        "averaged = tf.reduce_mean(embedded, axis=1)\n",
        "\n",
        "# Image part: Convolutional and pooling layers\n",
        "cov = Conv2D(32, (16, 16))(in_image)\n",
        "pl = MaxPool2D((16, 16))(cov)\n",
        "flattened = Flatten()(pl)\n",
        "\n",
        "# Fusion: Concatenating the outputs from text and image branches\n",
        "fused = tf.concat([averaged, flattened], axis=-1)\n",
        "\n",
        "# Multi-task learning: Dense layers for each task (type and price prediction)\n",
        "p_type = Dense(len_type, activation='softmax', name='type')(fused)\n",
        "p_price = Dense(len_price, activation='softmax', name='price')(fused)\n",
        "\n",
        "# Define model input/output using keys\n",
        "model = keras.Model(\n",
        "    inputs={\n",
        "        'summary': in_text,\n",
        "        'image': in_image\n",
        "    },\n",
        "    outputs={\n",
        "        'type': p_type,\n",
        "        'price': p_price,\n",
        "    },\n",
        ")\n",
        "\n",
        "# Compile the model with optimizer, loss values for each task, and loss weights for each task\n",
        "model.compile(\n",
        "    optimizer=Adam(),\n",
        "    loss={\n",
        "        'type': 'sparse_categorical_crossentropy',\n",
        "        'price': 'sparse_categorical_crossentropy',\n",
        "    },\n",
        "    loss_weights={\n",
        "        'type': 0.5,\n",
        "        'price': 0.5,\n",
        "    },\n",
        "    metrics={\n",
        "        'type': ['SparseCategoricalAccuracy'],\n",
        "        'price': ['SparseCategoricalAccuracy'],\n",
        "    },\n",
        ")\n",
        "\n",
        "# Print model summary\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Training\n",
        "\n",
        "Based on the training/validation performance, you can adjust the epochs to be trained. Early stoping is watching the validation loss on genre prediction (assuming that it is the main task we would like to perform)"
      ],
      "metadata": {
        "id": "UB-r6sfjBYh9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code trains the model using the fit method, which takes training data, labels, and various training parameters as inputs.\n",
        "The input data consists of text and image features, passed as a dictionary with keys 'summary' and 'image'.\n",
        "The target labels for the type and price tasks are provided as a dictionary with keys 'type' and 'price'.\n",
        "Training is performed for 5 epochs with a batch size of 50 samples.\n",
        "A fraction of the training data (20%) is used for validation during training to monitor model performance.\n",
        "An early stopping callback is included to monitor the validation loss ('val_type_loss') and stop training if the loss does not improve for 5 consecutive epochs.\n",
        "The training progress is displayed with a progress bar (verbose=1)."
      ],
      "metadata": {
        "id": "YZO8UTk4AyQx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "lyVFF4LTR1Zm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "daf49635-0a01-4baf-e4dc-f3373fa2c1b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "123/123 [==============================] - 114s 813ms/step - loss: 35.6415 - price_loss: 30.9827 - type_loss: 40.3004 - price_sparse_categorical_accuracy: 0.4922 - type_sparse_categorical_accuracy: 0.5738 - val_loss: 19.6999 - val_price_loss: 19.4168 - val_type_loss: 19.9831 - val_price_sparse_categorical_accuracy: 0.6258 - val_type_sparse_categorical_accuracy: 0.6933\n",
            "Epoch 2/5\n",
            "123/123 [==============================] - 71s 578ms/step - loss: 13.5687 - price_loss: 10.2912 - type_loss: 16.8461 - price_sparse_categorical_accuracy: 0.5086 - type_sparse_categorical_accuracy: 0.5806 - val_loss: 19.6382 - val_price_loss: 24.8529 - val_type_loss: 14.4236 - val_price_sparse_categorical_accuracy: 0.1029 - val_type_sparse_categorical_accuracy: 0.7588\n",
            "Epoch 3/5\n",
            "123/123 [==============================] - 77s 628ms/step - loss: 6.0495 - price_loss: 4.7521 - type_loss: 7.3469 - price_sparse_categorical_accuracy: 0.5050 - type_sparse_categorical_accuracy: 0.5955 - val_loss: 7.3147 - val_price_loss: 3.7287 - val_type_loss: 10.9007 - val_price_sparse_categorical_accuracy: 0.5944 - val_type_sparse_categorical_accuracy: 0.1330\n",
            "Epoch 4/5\n",
            "123/123 [==============================] - 79s 640ms/step - loss: 4.7474 - price_loss: 3.1739 - type_loss: 6.3210 - price_sparse_categorical_accuracy: 0.5411 - type_sparse_categorical_accuracy: 0.5981 - val_loss: 12.7209 - val_price_loss: 14.4074 - val_type_loss: 11.0343 - val_price_sparse_categorical_accuracy: 0.3139 - val_type_sparse_categorical_accuracy: 0.2195\n",
            "Epoch 5/5\n",
            "123/123 [==============================] - 73s 592ms/step - loss: 4.4248 - price_loss: 3.4470 - type_loss: 5.4025 - price_sparse_categorical_accuracy: 0.5258 - type_sparse_categorical_accuracy: 0.5924 - val_loss: 6.5922 - val_price_loss: 7.6291 - val_type_loss: 5.5553 - val_price_sparse_categorical_accuracy: 0.3434 - val_type_sparse_categorical_accuracy: 0.7582\n"
          ]
        }
      ],
      "source": [
        "# Train the model using the fit method\n",
        "history = model.fit(\n",
        "    x={\n",
        "        'summary': x_train_text_id,\n",
        "        'image': x_train_image\n",
        "    },\n",
        "    y={\n",
        "        'type': y_train_type,\n",
        "        'price': y_train_price,\n",
        "    },\n",
        "    epochs=5,  # Number of epochs for training\n",
        "    batch_size=50,  # Batch size for training\n",
        "    validation_split=0.2,  # Fraction of training data to use for validation\n",
        "    callbacks=[\n",
        "        # Early stopping callback to prevent overfitting\n",
        "        tf.keras.callbacks.EarlyStopping(monitor='val_type_loss', patience=5)\n",
        "    ],\n",
        "    verbose=1  # Verbosity mode (1 for progress bar)\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# trial 2: Image inputs. Cover at least once a Conv2d layer.\n",
        "The code defines batch input shapes for text and image inputs using keras.Input.\n",
        "For the text part, an Embedding layer is applied to the input text, followed by average pooling to summarize the text information.\n",
        "For the image part, two Convolutional layers and one max pooling layer are applied to extract image features.\n",
        "The outputs from the text and image branches are concatenated to fuse the information from both modalities.\n",
        "A Dense layer is added to predict the price of the listing.\n",
        "Model inputs and outputs are defined using keys to specify the input and output names.\n",
        "The model is compiled with an optimizer, loss function for the price task, and evaluation metrics.\n",
        "The model summary is printed to display the architecture and parameters of the model.\n",
        "The model is trained using the fit method, similar to the previous example, with 5 epochs, a batch size of 50, and early stopping callback to prevent overfitting."
      ],
      "metadata": {
        "id": "HboO3l0wBhJE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define batch input shapes for text and image inputs\n",
        "in_text = keras.Input(batch_shape=(None, max_len))\n",
        "in_image = keras.Input(batch_shape=(None, 64, 64, 2))\n",
        "\n",
        "# Text part: Embedding layer followed by average pooling\n",
        "embedded = keras.layers.Embedding(tokenizer.num_words, 100)(in_text)\n",
        "averaged = tf.reduce_mean(embedded, axis=1)\n",
        "\n",
        "# Image part: Convolutional and pooling layers\n",
        "cov = Conv2D(64, (16, 16))(in_image)\n",
        "cov = Conv2D(32, (16, 16))(cov)\n",
        "pl = MaxPool2D((16, 16))(cov)\n",
        "flattened = Flatten()(pl)\n",
        "\n",
        "# Fusion: Concatenating the outputs from text and image branches\n",
        "fused = tf.concat([averaged, flattened], axis=-1)\n",
        "\n",
        "# Price prediction: Dense layer for predicting the price\n",
        "p_price = Dense(len_price, activation='softmax', name='price')(fused)\n",
        "\n",
        "# Define model input/output using keys\n",
        "model2 = keras.Model(\n",
        "    inputs={\n",
        "        'summary': in_text,\n",
        "        'image': in_image\n",
        "    },\n",
        "    outputs={\n",
        "        'price': p_price,\n",
        "    },\n",
        ")\n",
        "\n",
        "# Compile the model with optimizer, loss function for the price task, and evaluation metrics\n",
        "model2.compile(\n",
        "    optimizer=Adam(),\n",
        "    loss={\n",
        "        'price': 'sparse_categorical_crossentropy',\n",
        "    },\n",
        "    metrics={\n",
        "        'price': ['SparseCategoricalAccuracy']\n",
        "    },\n",
        ")\n",
        "\n",
        "# Print model summary\n",
        "model2.summary()\n",
        "\n",
        "# Train the model2\n",
        "history2 = model2.fit(\n",
        "    x={\n",
        "        'summary': x_train_text_id,\n",
        "        'image': x_train_image\n",
        "    },\n",
        "    y={\n",
        "        'price': y_train_price,\n",
        "    },\n",
        "    epochs=5,  # Number of epochs for training\n",
        "    batch_size=50,  # Batch size for training\n",
        "    validation_split=0.2,  # Fraction of training data to use for validation\n",
        "    callbacks=[\n",
        "        # Early stopping callback to prevent overfitting\n",
        "        tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
        "    ],\n",
        "    verbose=1  # Verbosity mode (1 for progress bar)\n",
        ")\n"
      ],
      "metadata": {
        "id": "gVmgFs-ZYJZG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f163272-771b-4c7d-d6fd-79ada11d6a55"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_4 (InputLayer)        [(None, 64, 64, 2)]          0         []                            \n",
            "                                                                                                  \n",
            " conv2d_1 (Conv2D)           (None, 49, 49, 64)           32832     ['input_4[0][0]']             \n",
            "                                                                                                  \n",
            " input_3 (InputLayer)        [(None, 100)]                0         []                            \n",
            "                                                                                                  \n",
            " conv2d_2 (Conv2D)           (None, 34, 34, 32)           524320    ['conv2d_1[0][0]']            \n",
            "                                                                                                  \n",
            " embedding_1 (Embedding)     (None, 100, 100)             4000000   ['input_3[0][0]']             \n",
            "                                                                                                  \n",
            " max_pooling2d_1 (MaxPoolin  (None, 2, 2, 32)             0         ['conv2d_2[0][0]']            \n",
            " g2D)                                                                                             \n",
            "                                                                                                  \n",
            " tf.math.reduce_mean_1 (TFO  (None, 100)                  0         ['embedding_1[0][0]']         \n",
            " pLambda)                                                                                         \n",
            "                                                                                                  \n",
            " flatten_1 (Flatten)         (None, 128)                  0         ['max_pooling2d_1[0][0]']     \n",
            "                                                                                                  \n",
            " tf.concat_1 (TFOpLambda)    (None, 228)                  0         ['tf.math.reduce_mean_1[0][0]'\n",
            "                                                                    , 'flatten_1[0][0]']          \n",
            "                                                                                                  \n",
            " price (Dense)               (None, 3)                    687       ['tf.concat_1[0][0]']         \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 4557839 (17.39 MB)\n",
            "Trainable params: 4557839 (17.39 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/5\n",
            "123/123 [==============================] - 1253s 10s/step - loss: 531.3028 - sparse_categorical_accuracy: 0.4901 - val_loss: 21.1385 - val_sparse_categorical_accuracy: 0.1376\n",
            "Epoch 2/5\n",
            "123/123 [==============================] - 1272s 10s/step - loss: 5.3536 - sparse_categorical_accuracy: 0.5316 - val_loss: 4.7922 - val_sparse_categorical_accuracy: 0.5682\n",
            "Epoch 3/5\n",
            "123/123 [==============================] - 1267s 10s/step - loss: 3.0485 - sparse_categorical_accuracy: 0.5597 - val_loss: 8.4058 - val_sparse_categorical_accuracy: 0.2169\n",
            "Epoch 4/5\n",
            "123/123 [==============================] - 1232s 10s/step - loss: 4.7402 - sparse_categorical_accuracy: 0.5222 - val_loss: 4.2477 - val_sparse_categorical_accuracy: 0.3952\n",
            "Epoch 5/5\n",
            "123/123 [==============================] - 1220s 10s/step - loss: 3.7514 - sparse_categorical_accuracy: 0.5599 - val_loss: 3.7610 - val_sparse_categorical_accuracy: 0.5695\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# trial 3: Text inputs. Cover at least once a GRU/LSTM layer.\n",
        "The code defines a batch input shape for the text input using keras.Input.\n",
        "For the text part, an Embedding layer is applied to the input text, followed by an LSTM layer to capture sequential information.\n",
        "The output from the LSTM layer is passed through a Dense layer to predict the price of the listing.\n",
        "Model input and output are defined using keys to specify the input and output names.\n",
        "The model is compiled with an optimizer, loss function for the price task, and evaluation metrics.\n",
        "The model summary is printed to display the architecture and parameters of the model.\n",
        "The model is trained using the fit method, similar to the previous examples, with 5 epochs, a batch size of 30, and early stopping callback to prevent overfitting."
      ],
      "metadata": {
        "id": "3nCr9s7-CQKM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define batch input shape for text input\n",
        "in_text = keras.Input(batch_shape=(None, max_len))\n",
        "\n",
        "# Text part: Embedding layer followed by LSTM layer\n",
        "embedded = keras.layers.Embedding(tokenizer.num_words, 100)(in_text)\n",
        "p_price = LSTM(128, dropout=0.2, recurrent_dropout=0.2)(embedded)\n",
        "p_price = Dense(len_price, activation='softmax', name='price')(p_price)\n",
        "\n",
        "# Define model input/output using keys\n",
        "model3 = keras.Model(\n",
        "    inputs={\n",
        "        'summary': in_text,\n",
        "    },\n",
        "    outputs={\n",
        "        'price': p_price,\n",
        "    },\n",
        ")\n",
        "\n",
        "# Compile the model with optimizer, loss function for the price task, and evaluation metrics\n",
        "model3.compile(\n",
        "    optimizer=Adam(),\n",
        "    loss={\n",
        "        'price': 'sparse_categorical_crossentropy',\n",
        "    },\n",
        "    metrics={\n",
        "        'price': ['SparseCategoricalAccuracy']\n",
        "    },\n",
        ")\n",
        "\n",
        "# Print model summary\n",
        "model3.summary()\n",
        "\n",
        "# Train the model3\n",
        "history3 = model3.fit(\n",
        "    x={\n",
        "        'summary': x_train_text_id,\n",
        "    },\n",
        "    y={\n",
        "        'price': y_train_price,\n",
        "    },\n",
        "    epochs=5,  # Number of epochs for training\n",
        "    batch_size=30,  # Batch size for training\n",
        "    validation_split=0.2,  # Fraction of training data to use for validation\n",
        "    callbacks=[\n",
        "        # Early stopping callback to prevent overfitting\n",
        "        tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
        "    ],\n",
        "    verbose=1  # Verbosity mode (1 for progress bar)\n",
        ")\n"
      ],
      "metadata": {
        "id": "wjJT3VBRnHhD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "167b081f-4d03-4ef1-db91-4758ca05e979"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_5 (InputLayer)        [(None, 100)]             0         \n",
            "                                                                 \n",
            " embedding_2 (Embedding)     (None, 100, 100)          4000000   \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 128)               117248    \n",
            "                                                                 \n",
            " price (Dense)               (None, 3)                 387       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4117635 (15.71 MB)\n",
            "Trainable params: 4117635 (15.71 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/5\n",
            "204/204 [==============================] - 90s 420ms/step - loss: 0.8531 - sparse_categorical_accuracy: 0.6171 - val_loss: 0.8373 - val_sparse_categorical_accuracy: 0.6252\n",
            "Epoch 2/5\n",
            "204/204 [==============================] - 78s 382ms/step - loss: 0.8393 - sparse_categorical_accuracy: 0.6201 - val_loss: 0.8345 - val_sparse_categorical_accuracy: 0.6252\n",
            "Epoch 3/5\n",
            "204/204 [==============================] - 81s 397ms/step - loss: 0.8390 - sparse_categorical_accuracy: 0.6201 - val_loss: 0.8335 - val_sparse_categorical_accuracy: 0.6252\n",
            "Epoch 4/5\n",
            "204/204 [==============================] - 85s 416ms/step - loss: 0.8388 - sparse_categorical_accuracy: 0.6201 - val_loss: 0.8330 - val_sparse_categorical_accuracy: 0.6252\n",
            "Epoch 5/5\n",
            "204/204 [==============================] - 78s 381ms/step - loss: 0.8386 - sparse_categorical_accuracy: 0.6201 - val_loss: 0.8324 - val_sparse_categorical_accuracy: 0.6252\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# trial 4: Text inputs. Cover at least once a BiDirectional layer.\n",
        "The code defines a batch input shape for the text input using keras.Input.\n",
        "For the text part, an Embedding layer is applied to the input text with specified input length.\n",
        "The output from the Embedding layer is passed through a Bidirectional LSTM layer to capture bidirectional sequential information.\n",
        "The output from the Bidirectional LSTM layer is passed through a Dense layer to predict the price of the listing.\n",
        "Model input and output are defined using keys to specify the input and output names.\n",
        "The model is compiled with an optimizer, loss function for the price task, and evaluation metric.\n",
        "The model summary is printed to display the architecture and parameters of the model.\n",
        "The model is trained using the fit method, with 5 epochs, a batch size of 20, and early stopping callback to prevent overfitting.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "00YfFe9DCoLI"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LdypsInR7xs-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define batch input shape for text input\n",
        "in_text = keras.Input(batch_shape=(None, max_len))\n",
        "\n",
        "# Text part: Embedding layer followed by Bidirectional LSTM layer\n",
        "embedded = keras.layers.Embedding(tokenizer.num_words, 100, input_length=max_len)(in_text)\n",
        "p_price = Bidirectional(LSTM(64))(embedded)\n",
        "p_price = Dense(len_price, activation='softmax', name='price')(p_price)\n",
        "\n",
        "# Define model input/output using keys\n",
        "model4 = keras.Model(\n",
        "    inputs={\n",
        "        'summary': in_text,\n",
        "    },\n",
        "    outputs={\n",
        "        'price': p_price,\n",
        "    },\n",
        ")\n",
        "\n",
        "# Compile the model with optimizer, loss function for the price task, and evaluation metrics\n",
        "model4.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Print model summary\n",
        "model4.summary()\n",
        "\n",
        "# Train the model4\n",
        "history4 = model4.fit(\n",
        "    x_train_text_id,  # Input text data\n",
        "    y_train_price,  # Target price labels\n",
        "    validation_split=0.2,  # Fraction of training data to use for validation\n",
        "    epochs=5,  # Number of epochs for training\n",
        "    batch_size=20,  # Batch size for training\n",
        "    callbacks=[\n",
        "        # Early stopping callback to prevent overfitting\n",
        "        tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=4)\n",
        "    ],\n",
        "    verbose=1  # Verbosity mode (1 for progress bar)\n",
        ")\n"
      ],
      "metadata": {
        "id": "YRXOhlMEon9Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "caa5368f-3ae7-44f0-db1d-81d9e12a0b5b"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_6 (InputLayer)        [(None, 100)]             0         \n",
            "                                                                 \n",
            " embedding_3 (Embedding)     (None, 100, 100)          4000000   \n",
            "                                                                 \n",
            " bidirectional (Bidirection  (None, 128)               84480     \n",
            " al)                                                             \n",
            "                                                                 \n",
            " price (Dense)               (None, 3)                 387       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4084867 (15.58 MB)\n",
            "Trainable params: 4084867 (15.58 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/5\n",
            "306/306 [==============================] - 65s 200ms/step - loss: 0.8470 - accuracy: 0.6201 - val_loss: 0.8312 - val_accuracy: 0.6252\n",
            "Epoch 2/5\n",
            "306/306 [==============================] - 61s 198ms/step - loss: 0.8402 - accuracy: 0.6201 - val_loss: 0.8346 - val_accuracy: 0.6252\n",
            "Epoch 3/5\n",
            "306/306 [==============================] - 61s 199ms/step - loss: 0.8396 - accuracy: 0.6201 - val_loss: 0.8311 - val_accuracy: 0.6252\n",
            "Epoch 4/5\n",
            "306/306 [==============================] - 61s 200ms/step - loss: 0.8387 - accuracy: 0.6201 - val_loss: 0.8317 - val_accuracy: 0.6252\n",
            "Epoch 5/5\n",
            "306/306 [==============================] - 59s 193ms/step - loss: 0.8391 - accuracy: 0.6201 - val_loss: 0.8342 - val_accuracy: 0.6252\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# trial 5: Image inputs. Cover at least once a Conv2d layer.\n",
        "The code defines a batch input shape for the image input using keras.Input.\n",
        "For the image part, two Convolutional layers and one max pooling layer are applied to extract image features.\n",
        "The output from the max pooling layer is flattened to be passed through a Dense layer for predicting the price of the listing.\n",
        "Model input and output are defined using keys to specify the input and output names.\n",
        "The model is compiled with an optimizer, loss function for the price task, and evaluation metric.\n",
        "The model summary is printed to display the architecture and parameters of the model.\n",
        "The model is trained using the fit method, with 5 epochs, a batch size of 100, and early stopping callback to prevent overfitting."
      ],
      "metadata": {
        "id": "5cx-nExjDBkY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define batch input shape for image input\n",
        "in_image = keras.Input(batch_shape=(None, 64, 64, 2))\n",
        "\n",
        "# Image part: Convolutional and pooling layers\n",
        "cov = Conv2D(32, (16, 16))(in_image)\n",
        "cov = Conv2D(16, (16, 16))(cov)\n",
        "pl = MaxPool2D((16, 16))(cov)\n",
        "flattened = Flatten()(pl)\n",
        "\n",
        "# Price prediction: Dense layer for predicting the price\n",
        "p_price = Dense(len_price, activation='softmax', name='price')(flattened)\n",
        "\n",
        "# Define model input/output using keys\n",
        "model5 = keras.Model(\n",
        "    inputs={\n",
        "        'image': in_image\n",
        "    },\n",
        "    outputs={\n",
        "        'price': p_price,\n",
        "    },\n",
        ")\n",
        "\n",
        "# Compile the model with optimizer, loss function for the price task, and evaluation metric\n",
        "model5.compile(\n",
        "    optimizer=Adam(),\n",
        "    loss={\n",
        "        'price': 'sparse_categorical_crossentropy',\n",
        "    },\n",
        "    metrics={\n",
        "        'price': ['accuracy'],\n",
        "    },\n",
        ")\n",
        "\n",
        "# Print model summary\n",
        "model5.summary()\n",
        "\n",
        "# Train the model5\n",
        "history5 = model5.fit(\n",
        "    x_train_image,  # Input image data\n",
        "    y_train_price,  # Target price labels\n",
        "    validation_split=0.2,  # Fraction of training data to use for validation\n",
        "    epochs=5,  # Number of epochs for training\n",
        "    batch_size=100,  # Batch size for training\n",
        "    callbacks=[\n",
        "        # Early stopping callback to prevent overfitting\n",
        "        tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=4)\n",
        "    ],\n",
        "    verbose=1  # Verbosity mode (1 for progress bar)\n",
        ")\n"
      ],
      "metadata": {
        "id": "0lBuLzmmr0uO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d1750c3-0e84-4e29-cb74-f6ff78a7a824"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_7 (InputLayer)        [(None, 64, 64, 2)]       0         \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 49, 49, 32)        16416     \n",
            "                                                                 \n",
            " conv2d_4 (Conv2D)           (None, 34, 34, 16)        131088    \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPoolin  (None, 2, 2, 16)          0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " flatten_2 (Flatten)         (None, 64)                0         \n",
            "                                                                 \n",
            " price (Dense)               (None, 3)                 195       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 147699 (576.95 KB)\n",
            "Trainable params: 147699 (576.95 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/5\n",
            "62/62 [==============================] - 534s 9s/step - loss: 468.3077 - accuracy: 0.4819 - val_loss: 31.0543 - val_accuracy: 0.6206\n",
            "Epoch 2/5\n",
            "62/62 [==============================] - 523s 8s/step - loss: 17.0905 - accuracy: 0.5168 - val_loss: 16.1660 - val_accuracy: 0.6265\n",
            "Epoch 3/5\n",
            "62/62 [==============================] - 514s 8s/step - loss: 11.5972 - accuracy: 0.5168 - val_loss: 13.1287 - val_accuracy: 0.3349\n",
            "Epoch 4/5\n",
            "62/62 [==============================] - 523s 8s/step - loss: 7.2352 - accuracy: 0.5181 - val_loss: 5.9060 - val_accuracy: 0.5649\n",
            "Epoch 5/5\n",
            "62/62 [==============================] - 526s 9s/step - loss: 3.9529 - accuracy: 0.5594 - val_loss: 4.3531 - val_accuracy: 0.5052\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# trial 6: Image inputs. Cover at least once a Dropout layer.\n",
        "The code defines a batch input shape for the image input using keras.Input.\n",
        "For the image part, two Convolutional layers with ReLU activation and max pooling layers with dropout regularization are applied to extract image features.\n",
        "The output from the max pooling layers is flattened to be passed through a Dense layer for predicting the price of the listing.\n",
        "Model input and output are defined using keys to specify the input and output names.\n",
        "The model is compiled with an optimizer, loss function for the price task, and evaluation metric.\n",
        "The model summary is printed to display the architecture and parameters of the model.\n",
        "The model is trained using the fit method, with 5 epochs, a batch size of 50, and early stopping callback to prevent overfitting."
      ],
      "metadata": {
        "id": "a4yeymWnDaow"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define batch input shape for image input\n",
        "in_image = keras.Input(batch_shape=(None, 64, 64, 2))\n",
        "\n",
        "# Image part: Convolutional and pooling layers with dropout regularization\n",
        "cov = Conv2D(6, (5, 5), activation='relu')(in_image)\n",
        "pl = MaxPool2D(strides=2)(cov)\n",
        "drop = Dropout(0.25)(pl)\n",
        "cov = Conv2D(16, (5, 5), activation='relu')(drop)\n",
        "pl = MaxPool2D(strides=2)(cov)\n",
        "drop = Dropout(0.25)(pl)\n",
        "\n",
        "# Flatten the output for the dense layer\n",
        "flattened = Flatten()(drop)\n",
        "\n",
        "# Price prediction: Dense layer for predicting the price\n",
        "p_price = Dense(len_price, activation='softmax', name='price')(flattened)\n",
        "\n",
        "# Define model input/output using keys\n",
        "model6 = keras.Model(\n",
        "    inputs={\n",
        "        'image': in_image\n",
        "    },\n",
        "    outputs={\n",
        "        'price': p_price,\n",
        "    },\n",
        ")\n",
        "\n",
        "# Compile the model with optimizer, loss function for the price task, and evaluation metric\n",
        "model6.compile(\n",
        "    optimizer=Adam(),\n",
        "    loss={\n",
        "        'price': 'sparse_categorical_crossentropy',\n",
        "    },\n",
        "    metrics={\n",
        "        'price': ['accuracy'],\n",
        "    },\n",
        ")\n",
        "\n",
        "# Print model summary\n",
        "model6.summary()\n",
        "\n",
        "# Train the model6\n",
        "history6 = model6.fit(\n",
        "    x_train_image,  # Input image data\n",
        "    y_train_price,  # Target price labels\n",
        "    validation_split=0.2,  # Fraction of training data to use for validation\n",
        "    epochs=5,  # Number of epochs for training\n",
        "    batch_size=50,  # Batch size for training\n",
        "    callbacks=[\n",
        "        # Early stopping callback to prevent overfitting\n",
        "        tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=4)\n",
        "    ],\n",
        "    verbose=1  # Verbosity mode (1 for progress bar)\n",
        ")\n"
      ],
      "metadata": {
        "id": "EyyWgY0Ysh_u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "004acb91-e35e-48c2-d992-a9970afd2977"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_5\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_8 (InputLayer)        [(None, 64, 64, 2)]       0         \n",
            "                                                                 \n",
            " conv2d_5 (Conv2D)           (None, 60, 60, 6)         306       \n",
            "                                                                 \n",
            " max_pooling2d_3 (MaxPoolin  (None, 30, 30, 6)         0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 30, 30, 6)         0         \n",
            "                                                                 \n",
            " conv2d_6 (Conv2D)           (None, 26, 26, 16)        2416      \n",
            "                                                                 \n",
            " max_pooling2d_4 (MaxPoolin  (None, 13, 13, 16)        0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 13, 13, 16)        0         \n",
            "                                                                 \n",
            " flatten_3 (Flatten)         (None, 2704)              0         \n",
            "                                                                 \n",
            " price (Dense)               (None, 3)                 8115      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 10837 (42.33 KB)\n",
            "Trainable params: 10837 (42.33 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/5\n",
            "123/123 [==============================] - 20s 152ms/step - loss: 8.3784 - accuracy: 0.5261 - val_loss: 1.0056 - val_accuracy: 0.5813\n",
            "Epoch 2/5\n",
            "123/123 [==============================] - 18s 149ms/step - loss: 0.9232 - accuracy: 0.6092 - val_loss: 0.8408 - val_accuracy: 0.6245\n",
            "Epoch 3/5\n",
            "123/123 [==============================] - 18s 150ms/step - loss: 0.8439 - accuracy: 0.6184 - val_loss: 0.8347 - val_accuracy: 0.6245\n",
            "Epoch 4/5\n",
            "123/123 [==============================] - 18s 150ms/step - loss: 0.8410 - accuracy: 0.6186 - val_loss: 0.8335 - val_accuracy: 0.6239\n",
            "Epoch 5/5\n",
            "123/123 [==============================] - 18s 150ms/step - loss: 0.8383 - accuracy: 0.6201 - val_loss: 0.8329 - val_accuracy: 0.6245\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preprocessing (Testing)\n",
        "\n",
        "Here we use the trained tokenizer to pre-process the testing set."
      ],
      "metadata": {
        "id": "1Pa1wsZWBoFc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "6fxoXMDER1Zm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "22bb1dbc09814c539ba4059442ec5c57",
            "0289b6f876e94fb1ade15605cc2b3c62",
            "85cdc73c40ce477cae3ee438eb8e309c",
            "e82d3a4b5e824c239ce730274aee4a9e",
            "488d28c5c05a4f6aa3863577d2a7d4d4",
            "d1cc764c2d0a41e59cc24cabc79b6768",
            "951d06383af140829bad36b6cdc30f24",
            "84a92d28335b4bb283179e7f97cc8292",
            "cf8757e738b84fe491195b9758368bd0",
            "41d906fc511449b78a1b9dfb72639a32",
            "db943a0ce2e4497eaa8bfa483ce4a975"
          ]
        },
        "outputId": "59604e76-7121-4949-a26a-d485bbfea18a"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/7360 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "22bb1dbc09814c539ba4059442ec5c57"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(7360, 100)\n"
          ]
        }
      ],
      "source": [
        "# loading images:\n",
        "x_test_image = np.array([load_image(i) for i in tqdm(dftest.image)])\n",
        "\n",
        "# loading overview: (force convert some of the non-string cell to string)\n",
        "x_test_text = _preprocess(dftest.summary.astype('str'))\n",
        "\n",
        "# Load text data from the 'summary' column of the DataFrame 'df' and convert non-string cells to strings\n",
        "#x_train_text = df.summary.astype('str')\n",
        "\n",
        "# Initialize an empty list to store translated texts\n",
        "#translated_texts = []\n",
        "\n",
        "# Iterate through each text in x_test_text and translate it\n",
        "#for text in x_test_text:\n",
        "#    translated_texts.append(translate_text(text))\n",
        "\n",
        "# Assign the translated texts to x_test_text\n",
        "#x_test_text = translated_texts\n",
        "# Preprocess text data: convert text to sequences of integers and pad sequences\n",
        "#x_test_text_id = _preprocess(x_test_text)\n",
        "#x_test_text_id = _preprocess(x_test_text[\"summary\"])\n",
        "# Print the shape of preprocessed text data\n",
        "print(x_test_text.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Predition\n",
        "\n",
        "We can use the model to predict the testing samples."
      ],
      "metadata": {
        "id": "FdVbNitNBynm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# this fun to save the predicted price of testing as a csv file\n",
        "def save_csv(pred, file_name):\n",
        "    df = pd.DataFrame({\"id\": dftest.id ,\"price\": pred})\n",
        "    df.to_csv(f\"{file_name}.csv\", index=False)"
      ],
      "metadata": {
        "id": "C5fpHYwRUKwm"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "ZoqKB--AZYgc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8e16b56-955d-4c88-cb5c-91db74a88cbf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "230/230 [==============================] - 16s 69ms/step\n",
            "230/230 [==============================] - 210s 913ms/step\n",
            "230/230 [==============================] - 13s 56ms/step\n",
            "230/230 [==============================] - 7s 29ms/step\n",
            "230/230 [==============================] - 66s 284ms/step\n",
            "230/230 [==============================] - 8s 34ms/step\n",
            "[[1.3258837e-05 9.9998671e-01 1.2788259e-09]\n",
            " [7.9282343e-01 2.0717648e-01 5.1741989e-11]\n",
            " [4.8902479e-04 9.9951094e-01 1.4204990e-08]\n",
            " ...\n",
            " [1.4179170e-07 9.9999982e-01 3.4619756e-09]\n",
            " [7.5050998e-01 2.4948999e-01 2.3679636e-11]\n",
            " [9.6005434e-03 9.8994803e-01 4.5138050e-04]]\n",
            "[[9.9997228e-01 2.7673950e-05 5.7757910e-10]\n",
            " [9.9999994e-01 1.3739314e-08 4.6706646e-08]\n",
            " [8.2782745e-01 1.7215988e-01 1.2744612e-05]\n",
            " ...\n",
            " [1.9378642e-02 9.8062146e-01 3.7622822e-12]\n",
            " [9.9999374e-01 5.1899056e-06 1.0042072e-06]\n",
            " [9.9999994e-01 2.1903734e-09 1.6527097e-12]]\n",
            "[[0.6020493  0.33634767 0.06160309]\n",
            " [0.6020493  0.33634767 0.06160309]\n",
            " [0.6020493  0.33634767 0.06160309]\n",
            " ...\n",
            " [0.6020493  0.33634767 0.06160309]\n",
            " [0.6020493  0.33634767 0.06160307]\n",
            " [0.6020493  0.33634767 0.06160307]]\n",
            "[[0.63943696 0.3151352  0.0454279 ]\n",
            " [0.63943696 0.3151352  0.0454279 ]\n",
            " [0.63943696 0.3151352  0.0454279 ]\n",
            " ...\n",
            " [0.63943696 0.3151352  0.0454279 ]\n",
            " [0.63943696 0.31513515 0.0454279 ]\n",
            " [0.63943696 0.31513515 0.0454279 ]]\n",
            "[[8.3257043e-01 1.6742913e-01 3.8417392e-07]\n",
            " [8.5974866e-01 1.3256210e-01 7.6891980e-03]\n",
            " [9.9965537e-01 3.0638091e-04 3.8249338e-05]\n",
            " ...\n",
            " [9.9999923e-01 7.6573571e-07 4.3295101e-13]\n",
            " [1.4849626e-01 1.5042156e-08 8.5150379e-01]\n",
            " [9.9264884e-01 7.3510990e-03 7.4617832e-17]]\n",
            "[[0.6122302  0.31840867 0.06936116]\n",
            " [0.6122302  0.31840867 0.06936116]\n",
            " [0.6122302  0.31840867 0.06936116]\n",
            " ...\n",
            " [0.6122302  0.31840867 0.06936116]\n",
            " [0.6122302  0.31840867 0.06936116]\n",
            " [0.8956447  0.0882911  0.01606421]]\n",
            "[1 0 1 ... 1 0 1]\n",
            "[0 0 0 ... 1 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 0 0]\n",
            "[0 0 0 ... 0 2 0]\n",
            "[0 0 0 ... 0 0 0]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# we can do prediction on training set\n",
        "y_predict = model.predict(\n",
        "    {\n",
        "        'summary': x_test_text,\n",
        "        'image': x_test_image\n",
        "    }\n",
        ")\n",
        "\n",
        "y_predict2 = model2.predict(\n",
        "    {\n",
        "        'summary': x_test_text,\n",
        "        'image': x_test_image\n",
        "    }\n",
        ")\n",
        "y_predict3 = model3.predict(\n",
        "    {\n",
        "        'summary': x_test_text\n",
        "    }\n",
        ")\n",
        "y_predict4 = model4.predict(\n",
        "    {\n",
        "        'summary': x_test_text\n",
        "    }\n",
        ")\n",
        "y_predict5 = model5.predict(\n",
        "    {\n",
        "\n",
        "        'image': x_test_image\n",
        "    }\n",
        ")\n",
        "y_predict6 = model6.predict(\n",
        "    {\n",
        "\n",
        "        'image': x_test_image\n",
        "    }\n",
        ")\n",
        "\n",
        "# probabilities\n",
        "price_predicted = y_predict['price']\n",
        "print(price_predicted)\n",
        "\n",
        "price_predicted2 = y_predict2['price']\n",
        "print(price_predicted2)\n",
        "\n",
        "price_predicted3 = y_predict3['price']\n",
        "print(price_predicted3)\n",
        "\n",
        "price_predicted4 = y_predict4['price']\n",
        "print(price_predicted4)\n",
        "\n",
        "price_predicted5 = y_predict5['price']\n",
        "print(price_predicted5)\n",
        "\n",
        "price_predicted6 = y_predict6['price']\n",
        "print(price_predicted6)\n",
        "\n",
        "# categories\n",
        "pricecat_predicted = np.argmax(price_predicted, axis=1)\n",
        "print(pricecat_predicted)\n",
        "# save the output\n",
        "save_csv(pricecat_predicted, \"trial_1\")\n",
        "\n",
        "# categories\n",
        "pricecat_predicted2 = np.argmax(price_predicted2, axis=1)\n",
        "print(pricecat_predicted2)\n",
        "# save the output\n",
        "save_csv(pricecat_predicted2, \"trial_2\")\n",
        "\n",
        "# categories\n",
        "pricecat_predicted3 = np.argmax(price_predicted3, axis=1)\n",
        "print(pricecat_predicted3)\n",
        "# save the output\n",
        "save_csv(pricecat_predicted3, \"trial_3\")\n",
        "\n",
        "# categories\n",
        "pricecat_predicted4 = np.argmax(price_predicted4, axis=1)\n",
        "print(pricecat_predicted4)\n",
        "# save the output\n",
        "save_csv(pricecat_predicted4, \"trial_4\")\n",
        "\n",
        "# categories\n",
        "pricecat_predicted5 = np.argmax(price_predicted5, axis=1)\n",
        "print(pricecat_predicted5)\n",
        "# save the output\n",
        "save_csv(pricecat_predicted5, \"trial_5\")\n",
        "\n",
        "# categories\n",
        "pricecat_predicted6 = np.argmax(price_predicted6, axis=1)\n",
        "print(pricecat_predicted6)\n",
        "# save the output\n",
        "save_csv(pricecat_predicted6, \"trial_6\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eHUXFXX5R1Zm"
      },
      "outputs": [],
      "source": [
        "#  (if for kaggle competition and it is about genre prediction)\n",
        "#pd.DataFrame(\n",
        "#    {'id': df_test.id,\n",
        "#     'genre': genre_category_predicted}\n",
        "#).to_csv('sample_submission.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dSKS4pLDjedo"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.10"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "145c5789145446628272efe0505f35cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_01b03fbd9d1b4f889df21be28dfe5670",
              "IPY_MODEL_28d8043ae0684e45841c78b9fdce2a10",
              "IPY_MODEL_09446fec7b4f4695bda5d71d2a8c1ce8"
            ],
            "layout": "IPY_MODEL_7f0e5df515644b0d989b78af52cc1a5d"
          }
        },
        "01b03fbd9d1b4f889df21be28dfe5670": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6c65ebb44970417a96605d8f5b5d11ca",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_ffe6795d2f6e401b90a0e75d54396fd7",
            "value": "100%"
          }
        },
        "28d8043ae0684e45841c78b9fdce2a10": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6947f075224645e4a67dfaa90a9e0e5d",
            "max": 7627,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9f37c9f0b5e84c6c84c25a049388cf8d",
            "value": 7627
          }
        },
        "09446fec7b4f4695bda5d71d2a8c1ce8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_54faf5df85c642ff9404d62f5e760d43",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_e85897b112c34b339b2627bfa92516a6",
            "value": "‚Äá7627/7627‚Äá[01:22&lt;00:00,‚Äá117.76it/s]"
          }
        },
        "7f0e5df515644b0d989b78af52cc1a5d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6c65ebb44970417a96605d8f5b5d11ca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ffe6795d2f6e401b90a0e75d54396fd7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6947f075224645e4a67dfaa90a9e0e5d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9f37c9f0b5e84c6c84c25a049388cf8d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "54faf5df85c642ff9404d62f5e760d43": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e85897b112c34b339b2627bfa92516a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "22bb1dbc09814c539ba4059442ec5c57": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0289b6f876e94fb1ade15605cc2b3c62",
              "IPY_MODEL_85cdc73c40ce477cae3ee438eb8e309c",
              "IPY_MODEL_e82d3a4b5e824c239ce730274aee4a9e"
            ],
            "layout": "IPY_MODEL_488d28c5c05a4f6aa3863577d2a7d4d4"
          }
        },
        "0289b6f876e94fb1ade15605cc2b3c62": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d1cc764c2d0a41e59cc24cabc79b6768",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_951d06383af140829bad36b6cdc30f24",
            "value": "100%"
          }
        },
        "85cdc73c40ce477cae3ee438eb8e309c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_84a92d28335b4bb283179e7f97cc8292",
            "max": 7360,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cf8757e738b84fe491195b9758368bd0",
            "value": 7360
          }
        },
        "e82d3a4b5e824c239ce730274aee4a9e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_41d906fc511449b78a1b9dfb72639a32",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_db943a0ce2e4497eaa8bfa483ce4a975",
            "value": "‚Äá7360/7360‚Äá[01:33&lt;00:00,‚Äá107.42it/s]"
          }
        },
        "488d28c5c05a4f6aa3863577d2a7d4d4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d1cc764c2d0a41e59cc24cabc79b6768": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "951d06383af140829bad36b6cdc30f24": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "84a92d28335b4bb283179e7f97cc8292": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cf8757e738b84fe491195b9758368bd0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "41d906fc511449b78a1b9dfb72639a32": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "db943a0ce2e4497eaa8bfa483ce4a975": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}