{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "üåà **Character n-gram vs. Word n-gram**: Character n-gram represents sequences of characters of length n, while Word n-gram represents sequences of words of length n. Character n-grams tend to suffer more from the out-of-vocabulary (OOV) issue because they can generate a large number of unique n-grams, especially in languages with complex character systems.\n",
        "\n",
        "üåà **Stop word removal vs. Stemming**: Stop word removal involves filtering out common words (e.g., \"the\", \"is\", \"and\") from text, while stemming involves reducing words to their base or root form (e.g., \"running\" to \"run\"). Stop word removal is language-dependent because common words vary between languages, while stemming can be language-dependent as different languages may have different morphological rules.\n",
        "\n",
        "üåà **Tokenization techniques and language dependency**: Tokenization techniques can be language-dependent because different languages have different rules for word boundaries, punctuation, and special characters. For example, languages like Chinese and Japanese do not have explicit word delimiters like spaces, making tokenization more challenging.\n",
        "\n",
        "üåà **Count vectorizer vs. TF-IDF vectorizer**: Count vectorizer represents text documents as a matrix of word counts, while TF-IDF vectorizer represents documents as a matrix of TF-IDF (Term Frequency-Inverse Document Frequency) values, which weigh words based on their importance in the document and across the corpus. It may not be feasible to use all possible n-grams, especially for large corpora, as it can lead to a high-dimensional and sparse feature space. Instead, n-grams should be selected based on domain knowledge, experimentation, and considering computational resources. Using techniques like limiting the maximum and minimum document frequency or selecting the top n-grams based on frequency can help in selecting relevant n-grams.\n",
        "\n",
        "\n",
        "**Problem Definition:**\n",
        "The task is to predict whether a Reddit post is fake news or not based on its title. The input is the title text of the Reddit post, and the output is a binary classification indicating whether the post is fake news or not. The data mining function required is classification.\n",
        "\n",
        "**Challenges:**\n",
        "**Text Preprocessing:** Raw data requires preprocessing. Two different techniques will be explored: tokenization and lemmatization.\n",
        "**Imbalanced Classes:** The dataset may have imbalanced classes, requiring techniques such as oversampling or undersampling.\n",
        "**Model Selection and Tuning:** Models such as XGBoost will be used, and hyperparameter tuning will be conducted using grid search.\n",
        "\n",
        "\n",
        "**Ideal Solution:**\n",
        "1. **Text Preprocessing Techniques:**\n",
        "   - Tokenization: Splitting text into words or tokens.\n",
        "   - Lemmatization: Reducing words to their base or root form.\n",
        "   \n",
        "2.\n",
        "   - Character-level Vectorizer: Converts text into sequences of characters.\n",
        "   - Word-level Vectorizer: Converts text into sequences of words.\n",
        "   \n",
        "3. **Model Selection and Tuning:**\n",
        "   - XGBoost Classifier: A boosting algorithm for classification tasks.\n",
        "   - Hyperparameter Tuning Methods:\n",
        "     - Grid Search: Exhaustive search over a predefined grid of hyperparameters.\n",
        "     \n",
        "4. **Validation Set:**\n",
        "   - Utilize a validation set for hyperparameter tuning to avoid overfitting.\n",
        "   \n",
        "5. **Tuning Frequency:**\n",
        "   - Conduct hyperparameter tuning at least five times, each time exploring different combinations of preprocessing techniques, feature sets, models, and tuning methods.\n",
        "\n"
      ],
      "metadata": {
        "id": "jTcULRvJkI2s"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N0sm_Pm_Tjms",
        "outputId": "c9eae071-7451-4acc-f288-7dc6f13d7a7b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       id                                               text  label\n",
            "0  265723  A group of friends began to volunteer at a hom...      0\n",
            "1  284269  British Prime Minister @Theresa_May on Nerve A...      0\n",
            "2  207715  In 1961, Goodyear released a kit that allows P...      0\n",
            "3  551106  Happy Birthday, Bob Barker! The Price Is Right...      0\n",
            "4    8584  Obama to Nation: ËÅô\"Innocent Cops and Unarmed Y...      0\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn import preprocessing\n",
        "import numpy as np\n",
        "df = pd.read_csv(\"xy_train.csv\")\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N2yhhNIjUu6N",
        "outputId": "8feeb504-cf83-4fd3-e720-e3325b537f0e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 60000 entries, 0 to 59999\n",
            "Data columns (total 3 columns):\n",
            " #   Column  Non-Null Count  Dtype \n",
            "---  ------  --------------  ----- \n",
            " 0   id      60000 non-null  int64 \n",
            " 1   text    60000 non-null  object\n",
            " 2   label   60000 non-null  int64 \n",
            "dtypes: int64(2), object(1)\n",
            "memory usage: 1.4+ MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n2HFgidtVBwU",
        "outputId": "a48282ae-11ed-4f44-9c0e-af967b2dc2be"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the distribution of the class labels, we can conclude several things:\n",
        "\n",
        "**Class Imbalance**: There is a class imbalance in the dataset. Class 0 has a significantly higher count (13797) compared to Class 1 (8932) and Class 2 (129).\n",
        "\n",
        "so, we decided to drop class 2 from the dataset because it isnot represent a real class"
      ],
      "metadata": {
        "id": "EC92fAf4G_07"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"label\"].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y3BTKzwPVJPx",
        "outputId": "6bf6310b-2fc6-479f-8d44-14bbd1b3ee44"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "label\n",
              "0    32172\n",
              "1    27596\n",
              "2      232\n",
              "Name: count, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A histogram for the class labels represents the distribution of each class within the dataset. In the context of your dataset with three classes (Class 0, Class 1, and Class 2),\n",
        "\n",
        "Here's what the graph represents:\n",
        "\n",
        " **Class Distribution**: The height of each bar in the histogram represents the count or frequency of instances belonging to each class in the dataset. Class 0 has 13797 instances, Class 1 has 8932 instances, and Class 2 has 129 instances, the histogram has three bars of different heights corresponding to these counts.\n",
        "\n",
        " **Class Imbalance**: Histograms are particularly useful for identifying class imbalance issues within the dataset. Class imbalance occurs when one or more classes are significantly underrepresented compared to others. In our case, Class 2 has a much smaller count compared to Class 0 and Class 1, it is represented by a shorter bar in the histogram, indicating class imbalance.\n"
      ],
      "metadata": {
        "id": "b2-TEInFHlwf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "sns.countplot(data = df , x= df['label'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466
        },
        "id": "fAHmj9gxVWyo",
        "outputId": "252d97fb-499a-4359-caa2-35ec819790ee"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Axes: xlabel='label', ylabel='count'>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#dropping class 2\n",
        "df = df[df[\"label\"] < 2]\n",
        "#df[\"label\"] = df[\"label\"].apply(lambda x: 1 if x==2 else x)"
      ],
      "metadata": {
        "id": "0ojkca4bVroZ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " the histogram after dropping Class 2 would provide a visual representation of the distribution of the remaining classes in the dataset. It would help to understand how removing Class 2 affects the composition of the data and the balance between the remaining classes.\n",
        "\n",
        " and after seeing the new histogram we can conclude now that this data is well distributed."
      ],
      "metadata": {
        "id": "prt8BmV_IZT9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import seaborn as sns\n",
        "sns.countplot(data = df , x= df['label'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466
        },
        "id": "bzgXOp1M4Hx0",
        "outputId": "876d345a-fdf4-462b-aceb-c378717b1f25"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Axes: xlabel='label', ylabel='count'>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#checking for labels again\n",
        "df[\"label\"].value_counts()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jazXGphGV3vX",
        "outputId": "0c227c3c-85f8-4700-cfeb-6df3ecaa18a4"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "label\n",
              "0    32172\n",
              "1    27596\n",
              "Name: count, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#datasize after removing class 2\n",
        "df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WxREhXvSV6X0",
        "outputId": "77813d04-4bcd-4369-d2f0-ac28b5fea43e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(59768, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# check for nulls\n",
        "no nulls"
      ],
      "metadata": {
        "id": "1DWOtWcrI2r-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "null_columns = df.isnull().any()\n",
        "print(null_columns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UZ4SrFhsWGE9",
        "outputId": "0cf71549-5d17-4314-9531-502314d0336d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "id       False\n",
            "text     False\n",
            "label    False\n",
            "dtype: bool\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# check for duplicates\n",
        "no duplicates"
      ],
      "metadata": {
        "id": "k7X2t5yqI5ZI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "duplicate_rows = df.duplicated()\n",
        "num_duplicates = duplicate_rows.sum()\n",
        "if num_duplicates > 0:\n",
        "    print(\"Duplicate rows:\")\n",
        "    print(df[duplicate_rows])\n",
        "else:\n",
        "    print(\"No duplicate rows found.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qAIKi9ilWhkC",
        "outputId": "d49def7c-84f6-4c2d-b631-dfd2bab4174b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No duplicate rows found.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code snippet imports essential libraries for data manipulation, natural language processing, and visualization. It defines a text cleaning function named `cleantext()`, which takes a text input and performs a series of preprocessing steps. These steps include removing URLs, HTML tags, non-alphabetic characters, single characters, and extra white spaces from the text. Additionally, the function tokenizes the text into words, converts them to lowercase, and applies lemmatization to reduce words to their base form. Furthermore, the function optionally handles stemming and stop words removal based on the value of the `embedding` parameter."
      ],
      "metadata": {
        "id": "Hl2rHJrOJUba"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re  # Importing the regular expression module for text cleaning\n",
        "import pickle  # Importing the pickle module for saving and loading Python objects\n",
        "import sklearn  # Importing the scikit-learn library for machine learning algorithms\n",
        "import pandas as pd  # Importing pandas library for data manipulation and analysis\n",
        "import numpy as np  # Importing numpy library for numerical computations\n",
        "import holoviews as hv  # Importing holoviews library for visualization\n",
        "import nltk  # Importing nltk library for natural language processing tasks\n",
        "from nltk.stem.snowball import SnowballStemmer  # Importing SnowballStemmer for word stemming\n",
        "from nltk.stem import WordNetLemmatizer  # Importing WordNetLemmatizer for word lemmatization\n",
        "from nltk.tokenize import word_tokenize  # Importing word_tokenize for tokenization\n",
        "from nltk.corpus import stopwords  # Importing stopwords from nltk corpus\n",
        "nltk.download('punkt')  # Downloading the punkt tokenizer models\n",
        "nltk.download('stopwords')  # Downloading the stopwords corpus\n",
        "nltk.download(\"all\")  # Downloading all nltk data (not typically recommended due to large size)\n",
        "\n",
        "# Initializing SnowballStemmer for English language\n",
        "stemmer = SnowballStemmer(\"english\")\n",
        "\n",
        "# Creating a set of English stopwords for text cleaning\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "# Initializing WordNetLemmatizer for word lemmatization\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def cleantext(text, embedding=False):\n",
        "    # Compiling regular expressions for cleaning text\n",
        "    links = re.compile(r'https?://\\S+', re.IGNORECASE)  # Regular expression to match URLs\n",
        "    Wspace = re.compile(r\"\\s+\", re.IGNORECASE)  # Regular expression to match white spaces\n",
        "    tags = re.compile(r\"<[^>]+>\")  # Regular expression to match HTML tags\n",
        "    ASCII = re.compile(r\"[^A-Za-z ]\", re.IGNORECASE)  # Regular expression to match non-alphabetic characters\n",
        "    singlechar = re.compile(r\"\\b[A-Za-z]\\b\", re.IGNORECASE)  # Regular expression to match single characters\n",
        "\n",
        "    # Handling punctuation and stop words if embedding is True\n",
        "    if embedding:\n",
        "        # Keep punctuation by modifying ASCII regex\n",
        "        ASCII = re.compile(r\"[^A-Za-z,.!? ]\", re.IGNORECASE)\n",
        "        # Keep single characters by modifying singlechar regex\n",
        "        singlechar = re.compile(r\"\\b[A-Za-z,.!?]\\b\", re.IGNORECASE)\n",
        "\n",
        "    # Cleaning text by substituting patterns with specified replacements\n",
        "    text = re.sub(links, \"link\", text)  # Substituting URLs with \"link\"\n",
        "    text = re.sub(tags, \" \", text)  # Removing HTML tags\n",
        "    text = re.sub(ASCII, \" \", text)  # Removing non-alphabetic characters\n",
        "    text = re.sub(singlechar, \" \", text)  # Removing single characters\n",
        "    text = re.sub(Wspace, \" \", text)  # Removing extra white spaces\n",
        "\n",
        "    # Tokenizing text into words\n",
        "    tokens = word_tokenize(text)\n",
        "    # Converting tokens to lowercase\n",
        "    tokens_lower = [word.lower() for word in tokens]\n",
        "\n",
        "    # Handling stemming, lemmatization, and stop words removal based on the value of embedding\n",
        "    if embedding:\n",
        "        # No stemming, lowering, and punctuation / stop words removal\n",
        "        words_filtered = tokens\n",
        "    else:\n",
        "        # Applying lemmatization and stop words removal\n",
        "        words_filtered = [lemmatizer.lemmatize(word) for word in tokens_lower if word not in stop_words]\n",
        "\n",
        "    # Joining filtered words back into a clean text string\n",
        "    text_clean = \" \".join(words_filtered)\n",
        "    return text_clean  # Returning the cleaned text\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xcIGjWjJWqDX",
        "outputId": "52dd2094-ede6-430b-f9eb-e9a7c0453c40"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n",
            "[nltk_data]    | Downloading package bcp47 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dolch.zip.\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n",
            "[nltk_data]    | Downloading package extended_omw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/floresta.zip.\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/indian.zip.\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/kimmo.zip.\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/moses_sample.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/paradigms.zip.\n",
            "[nltk_data]    | Downloading package pe08 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pe08.zip.\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/perluniprops.zip.\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pil.zip.\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pl196x.zip.\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ptb.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Package punkt is already up-to-date!\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/qc.zip.\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/rslp.zip.\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/rte.zip.\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/smultron.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Package stopwords is already up-to-date!\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/switchboard.zip.\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet.zip.\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet2022 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet2022.zip.\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ycoe.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#first line before cleaning\n",
        "df.iloc[0][\"text\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "id": "hSr3XvQ2Y94m",
        "outputId": "83c33ca1-a3ae-4393-bb4f-4b0d9df61bf1"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'A group of friends began to volunteer at a homeless shelter after their neighbors protested. \"Seeing another person whoÈà•Ê™ö also in need, you should just naturally be like, Èà•Ê•≠ want to help that person.Èà•?\\t0.84\\t1\\t0\\t0\\nNeoMegaRyuMKII\\tdenmark just trolled trump with a bus watch what happens when it moves\\t1489962542.0\\tarchive.is\\tTrue\\t60cu90\\thttps://external-preview.redd.it/s8HQxoWDwn2pt3eeBjIqetmGxvVMBBxaD93W08lGZrs.jpg?width=320&crop=smart&auto=webp&s=46f0ad18132126cab9823da4f3a6d6ec3a281bb2\\t\\t5.0\\t36\\tsavedyouaclick\\tDenmark Just Trolled Trump With A Bus. Watch What Happens When It Moves | Back wheels look like eyes that spin crazily as the bus moves (saved a click + short video that goes into slow motion (video mirror in comments since archive removed it))\\t0.92\\t0\\t2\\t5\\natbthefirst\\tyoung man theres no need to feel down\\t1429563057.0\\t\\tTrue\\tcqiw655\\thttp://i.imgur.com/rxTKm4U.jpg\\t338ute\\t\\t6\\tpsbattle_artwork\\tYoung man, there\\'s no need to feel down\\t\\t0\\t2\\t4\\nbeosronlto\\tmy onion ring chimp\\t1491483860.0\\timgur.com\\tTrue\\t63ss2x\\thttps://external-preview.redd.it/5muewXGrxJ6YS_iYpjqiBiXpIdxR7d7Woyyln7E4Xe4.jpg?width=320&crop=smart&auto=webp&s=68f0308f16bf0aa9dab6a28260dd06c85ed67450\\t\\t0.0\\t2\\tpareidolia\\tMy onion ring chimp\\t0.63\\t0\\t2\\t2\\nfood_SS\\tthis is what we eat in the images description\\t1443409099.0\\timgur.com\\tTrue\\t3mnjdi\\thttps://external-preview.redd.it/lLESgQetBjASylhfFUw6CKMfbKdR00WZA_LDVdXpOog.jpg?width=320&crop=smart&auto=webp&s=0cb7c577cf8f78d74b23db11f0069d3f32248d31\\t\\t20.0\\t20\\tsubredditsimulator\\tThis is what we eat in the images description)\\t0.84\\t0\\t2\\t3\\nCardboardSoyuz\\tarizona man purchases home and discovers an underground hatch in backyard\\t1505215025.0\\tweb.archive.org\\tTrue\\t6zmas6\\thttps://external-preview.redd.it/FBVXMNmhDNTqADXsiP9UE-NxQ7Caq5kTInsgl-BSvWI.jpg?width=320&crop=smart&auto=webp&s=cdf12b2b558aed4d0ad86199ea3d9592ff1f88c2\\t\\t7.0\\t613\\tsavedyouaclick\\tArizona Man Purchases Home And Discovers An Underground Hatch In Backyard | It leads to an old fall-out shelter in pretty good shape. It had a couple of cans of supplies in it, too. 17 clicks.\\t0.98\\t0\\t2\\t5\\nHarry_Butz\\tthe rpics awards spell out penis\\t1565605730.0\\ti.redd.it\\tTrue\\tcpajkp\\thttps://preview.redd.it/w948d5n4wzf31.jpg?width=320&crop=smart&auto=webp&s=a165988b2d70ef55257a8eeaeedf51b710f1a55b\\t\\t3.0\\t10\\tmildlyinteresting\\tThe /r/Pics awards spell out penis\\t0.71\\t1\\t0\\t0\\nbillarastg\\ttime traveller assassin attemps to take down obama before he becomes president circa\\t1551597882.0\\ti.redd.it\\tTrue\\tawrrxk\\thttps://preview.redd.it/behjedplvuj21.jpg?width=320&crop=smart&auto=webp&s=42ad1a826403a3575fbb94e823d47999b34cfbbd\\t\\t6.0\\t28\\tfakehistoryporn\\tTime traveller Assassin attemps to take down Obama before he becomes president. (circa 1952).\\t0.87\\t0\\t2\\t2\\nMissMaria86\\tew\\t1407262199.0\\t\\tTrue\\tcjhqf8a\\thttp://i.imgur.com/YBE2jms.jpg\\t2coipv\\t\\t95\\tpsbattle_artwork\\tEw.\\t\\t0\\t2\\t4\\nsnlisha\\tislamic state fighters reenter ancient palmyra in syria\\t1481392782.0\\tbbc.com\\tTrue\\t5hlch8\\thttps://external-preview.redd.it/rofxJBXObS76fw5nPTXg4tT9OXFHK-kvH33bZQyaO_8.jpg?width=320&crop=smart&auto=webp&s=02cc00050036279a3ae533ba8540830043fff74a\\t\\t0.0\\t5\\tneutralnews\\tIslamic State fighters \\'re-enter ancient Palmyra\\' in Syria\\t0.79\\t1\\t0\\t0\\nRaidyRafy\\ttop golf red line warning sign how many people have gotten hurt\\t1568605849.0\\ti.redd.it\\tTrue\\td4vcq7\\thttps://preview.redd.it/xtjcu5f0pvm31.jpg?width=320&crop=smart&auto=webp&s=9562ffe4127a0d6d34360e4df8a81e05de126519\\t\\t4.0\\t12\\tmildlyinteresting\\tTop Golf red line warning sign. How many people have gotten hurt?\\t0.83\\t1\\t0\\t0\\nBudrickBundy\\tthe nfls ratings slide was mainly fueled by white viewers and younger viewers\\t1516117668.0\\tawfulannouncing.com\\tTrue\\t7qt5fc\\thttps://external-preview.redd.it/vq1q3olqrN3tSCHnl2XJm349y4aGAnSEyepo6LkyFYQ.jpg?width=320&crop=smart&auto=webp&s=38bc55460c5785dbfb3dc2932c1e0321a1fe626c\\t\\t17.0\\t5\\tusanews\\tThe NFL\\'s 2017 ratings slide was mainly fueled by white viewers and younger viewers\\t0.69\\t1\\t0\\t0\\nbbizzess\\tmeryl streep marries\\t1475121888.0\\tarchive.is\\tTrue\\t550gen\\thttps://external-preview.redd.it/QaioECFejw0fZN8SE5WoeGkfu3rRHO3fbYKq_4Ae3b4.jpg?width=320&crop=smart&auto=webp&s=8a4d0ebe34e0e8f3b3b4f163a8369c92e1fb2c14\\t\\t1.0\\t34\\tsavedyouaclick\\tMeryl Streep Marries | Article says she\\'s marrying Robert Redford. She is not.\\t0.94\\t0\\t2\\t5\\nApiContraption\\tother discussions\\t1393550555.0\\t\\tTrue\\tcfqkht6\\thttp://i.imgur.com/XZObVs4%2ejpg\\t1z5152\\t\\t1\\tpsbattle_artwork\\tOther Discussions\\t\\t0\\t2\\t4\\nAskesl\\tsquash dingres\\t1447949876.0\\t\\tTrue\\tcx5t9jt\\thttp://i.imgur.com/4WrG3N6.jpg\\t3tfgjo\\t\\t242\\tpsbattle_artwork\\tSquash d\\'Ingres\\t\\t0\\t2\\t4\\nGrahamSaysNO\\tthis cactus man\\t1409262115.0\\timgur.com\\tTrue\\t2ev0r6\\thttps://external-preview.redd.it/tod214DPHe8fopeWeF_9MJUVbRXDR2tr78uzMrZSa2E.jpg?width=320&crop=smart&auto=webp&s=e6486a1ed49915388d5b756ae11b4f5e88bc64c4\\t\\t6.0\\t54\\tpareidolia\\tThis cactus man.\\t0.91\\t0\\t2\\t2\\nIShitPoopsALot\\troommates stab each other over debate between iphone and samsung\\t1429265409.0\\tktul.com\\tTrue\\t32wpnv\\thttps://external-preview.redd.it/TypZIEwJmWacgv45TjxxCrHXytjUCkLmhYjZUZatYmU.jpg?width=320&crop=smart&auto=webp&s=4cef96c45cc0f50e2d64f829daa184449136b89e\\t\\t183.0\\t1175\\tnottheonion\\tRoommates stab each other over debate between iPhone and Samsung.\\t0.93\\t1\\t0\\t0\\nslithek\\tgot rid of the ant\\t1365912044.0\\t\\tTrue\\tc9es9hx\\thttp://i.imgur.com/HOCK1nE.jpg\\t1cairr\\t\\t99\\tpsbattle_artwork\\tGot rid of the ant...\\t\\t0\\t2\\t4\\nJJ935\\ttitanic\\t1557350663.0\\t\\tTrue\\temv7vw0\\thttps://i.imgur.com/WIYL8aT.jpg\\tbm9yhe\\t\\t3\\tpsbattle_artwork\\tTitanic\\t\\t0\\t2\\t4\\nThigira\\tdeclaration of independence philadelphia\\t1553375330.0\\ti.redd.it\\tTrue\\tb4o9wf\\thttps://preview.redd.it/9v1epg3voxn21.jpg?width=320&crop=smart&auto=webp&s=eece10bb46d039890257c34ace5de65900928c13\\t\\t3.0\\t81\\tfakehistoryporn\\tDeclaration of Independence. Philadelphia, 1776. Colorized\\t0.91\\t0\\t2\\t2\\nBobblee20\\tthis cloud formation against a sunrise looks like a fire tornado\\t1551025203.0\\ti.redd.it\\tTrue\\tau9fsm\\thttps://preview.redd.it/f6kfxc7qkji21.jpg?width=320&crop=smart&auto=webp&s=bd863086c477286b4c2cdb4ca3fa0b547494c118\\t\\t1.0\\t48\\tmildlyinteresting\\tThis cloud formation against a sunrise looks like a fire tornado\\t0.93\\t1\\t0\\t0\\nChispy\\tobligatory faceswap\\t1472415912.0\\t\\tTrue\\td70cdmk\\thttp://i.imgur.com/KbRXBLs.jpg\\t4zzhjk\\t\\t60\\tpsbattle_artwork\\tobligatory faceswap\\t\\t0\\t2\\t4\\nVt412\\texcited dog\\t1367321385.0\\ti.imgur.com\\tTrue\\t1dekok\\thttps://external-preview.redd.it/ESPM-tTuK3werLgfWG_wBsw_trUSDNU4bI5s2jVRAgQ.png?width=108&crop=smart&auto=webp&s=6bdf4d9c8ec6b3100f803c1cf0848f49a5aee344\\t\\t1.0\\t1\\tphotoshopbattles\\tExcited dog\\t0.53\\t1\\t0\\t0\\nratchetraccoon\\tphotograph of first interracial family in pennsylvania after interracial marriage is legalized circa\\t1524449884.0\\ti.redd.it\\tTrue\\t8e84ku\\thttps://preview.redd.it/mydnsd5oikt01.jpg?width=320&crop=smart&auto=webp&s=dbcf91c9c5aacb9f49c2e2cdf5f9f8dcca3b7d8d\\t\\t4.0\\t103\\tfakehistoryporn\\tColorized photograph of first interracial family in Pennsylvania after interracial marriage is legalized, circa 1780\\t0.97\\t0\\t2\\t2\\nMrtenpence\\tmy local bus has usb chargers in every seat\\t1537876887.0\\timgur.com\\tTrue\\t9iri2s\\thttps://external-preview.redd.it/8OSU-4_TeshCI8i0-Qokoj4-GnuIeAsl1uS7Y5-yAko.jpg?width=320&crop=smart&auto=webp&s=ccbf8c98e0c00fb8f459a421a4a3d5f29dc05a18\\t\\t32.0\\t261\\tmildlyinteresting\\tMy local bus has USB chargers in every seat\\t0.92\\t1\\t0\\t0\\nLucastWaddle\\tthe fact that people keep buying this stuff at the ace harware i work at in texas\\t1561857484.0\\ti.redd.it\\tTrue\\tc7790i\\thttps://preview.redd.it/uj09xvsnae731.jpg?width=320&crop=smart&auto=webp&s=9b174799db2c906c646b9a8999e76d4dedc97a09\\t\\t4.0\\t7\\tmildlyinteresting\\tThe fact that people keep buying this stuff at the Ace Harware I work at in Texas.\\t0.82\\t1\\t0\\t0\\nOriginalstix\\thuge tennis racket or mini federer\\t1451981052.0\\t\\tTrue\\tcymkq8f\\thttp://i.imgur.com/kWNXbnZ.jpg\\t3zfune\\t\\t1\\tpsbattle_artwork\\thuge tennis racket or mini federer?\\t\\t0\\t2\\t4\\nmangobutter6179\\tthese cigs in mexico have a photo of a dead body on it also picked it up at a pharmacy who was selling it secretly lol\\t1568731999.0\\ti.redd.it\\tTrue\\td5i3v2\\thttps://preview.redd.it/jy5rsoi446n31.jpg?width=320&crop=smart&auto=webp&s=30c4d1ab9ff12513839b3492606c02d4dad3fedf\\t\\t7.0\\t8\\tmildlyinteresting\\tThese cigs in Mexico have a photo of a dead body on it, also picked it up at a pharmacy who was selling it secretly lol\\t0.75\\t1\\t0\\t0\\n9elefanttwoothpaste7\\tfat man bomb dropped on nagasaki\\t1501655233.0\\ti.imgur.com\\tTrue\\t6r2cz7\\thttps://external-preview.redd.it/Pk13Js7E_zB5abguKu7q7vJkzSol8soRomMY2bQsOcU.jpg?width=320&crop=smart&auto=webp&s=1846378b13372ae5a072f7c7494bc15b9243288d\\t\\t0.0\\t4\\tmisleadingthumbnails\\tFat Man bomb dropped on Nagasaki, 1945 (colorized)\\t0.57\\t0\\t2\\t2\\nViperdream\\tdeath is close\\t1364940402.0\\t\\tTrue\\tc978mws\\thttp://i.imgur.com/vW3hyor.jpg\\t1biplz\\t\\t1\\tpsbattle_artwork\\tDeath is close...\\t\\t0\\t2\\t4\\nMohhh777\\tbritish soldiers during a last stand in the battle of el alamein\\t1520350464.0\\ti.redd.it\\tTrue\\t82fx19\\thttps://preview.redd.it/q0pxntkzw5k01.jpg?width=320&crop=smart&auto=webp&s=f5e3a60c7d11e99167a41d139f7aafadfd572eab\\t\\t1.0\\t6\\tfakehistoryporn\\tBritish soldiers during a last stand in the battle of el alamein (1942,colorized)\\t0.87\\t0\\t2\\t2\\nWaffleSmoof\\ta year old doingthis\\t1466958003.0\\ti.imgur.com\\tTrue\\t4pyfx6\\thttps://external-preview.redd.it/E5eDuxcbUIShmnalJeNwxFdygee8fD8wnPkva04grGk.jpg?width=320&crop=smart&auto=webp&s=a55dceaa0d3d8be7b860c503b3ebbe881b2c4de9\\t\\t8.0\\t5\\tphotoshopbattles\\tPsBattle: A 4-year old doing...this\\t0.86\\t1\\t0\\t0\\nj_Wlms\\tboov\\t1480133670.0\\t\\tTrue\\tdafx4a6\\thttp://i64.tinypic.com/2iqi36.jpg\\t5etd1l\\t\\t3\\tpsbattle_artwork\\tboov\\t\\t0\\t2\\t4\\nLordMetrognome\\tthese puppies\\t1560574409.0\\ti.redd.it\\tTrue\\tc0tmcc\\thttps://preview.redd.it/cfv54djfbg431.jpg?width=320&crop=smart&auto=webp&s=4e3def0be13bf80708e56b7d1d787da9dc992953\\t\\t6.0\\t35\\tphotoshopbattles\\tPsBattle: These 3 puppies\\t0.85\\t1\\t0\\t0\\nswigsweg8897\\twe raced another plane on my flight back from europe the other day\\t1536676609.0\\timgur.com\\tTrue\\t9ey3ng\\thttps://external-preview.redd.it/gG22KFjIVjNARSyZPlTpL34Fs_O-IJ-Po07K_4JCkBA.jpg?width=320&crop=smart&auto=webp&s=21d02ead80ce93e5a3066d4285bd580a3d98d700\\t\\t5.0\\t13\\tmildlyinteresting\\tWe raced another plane on my flight back from Europe the other day\\t0.85\\t1\\t0\\t0\\nBlackjack115\\ttexas woman uses her coupon clipping skills to help hurricane survivors\\t1505681867.0\\tabcnews.go.com\\tTrue\\t70q7na\\thttps://external-preview.redd.it/X34E8Gwjznfyqi_wuBHow-e6zi3caqVn8IQ8zWx82CU.jpg?width=320&crop=smart&auto=webp&s=ba32edcc58727c1de53e72a37fd51aeae0ef9887\\t\\t615.0\\t29035\\tupliftingnews\\tTexas woman uses her coupon clipping skills to help hurricane survivors\\t0.87\\t1\\t0\\t0\\nApiContraption\\tcutouts\\t1412348760.0\\t\\tTrue\\tckzeba8\\thttp://i.imgur.com/uSH1Cwe%2ejpg\\t2i6ymd\\t\\t2\\tpsbattle_artwork\\tcutouts\\t\\t0\\t2\\t4\\njargos\\tidk might be nsfw\\t1373170717.0\\t\\tTrue\\tcaxeexm\\thttp://i.imgur.com/B9oD0ac.jpg\\t1hs7jh\\t\\t15\\tpsbattle_artwork\\tIDK, might be NSFW\\t\\t0\\t2\\t4\\nene_due_rabe\\tdeep inside shes no different\\t1524409377.0\\t\\tTrue\\tdxs4e1p\\thttps://i.imgur.com/pQnFFRS.jpg\\t8e2py4\\t\\t3\\tpsbattle_artwork\\tDeep inside she\\'s no different\\t\\t0\\t2\\t4\\nRizilus\\tbig head\\t1524535418.0\\t\\tTrue\\tdxv3mcd\\thttps://i.imgur.com/Kfo4au9.jpg\\t8ee4tt\\t\\t2\\tpsbattle_artwork\\tBig Head\\t\\t0\\t2\\t4\\nchibucks\\teveryone looks so yummy\\t1445894162.0\\t\\tTrue\\tcwdtq0c\\thttp://i.imgur.com/FA7Tni2.jpg\\t3q9z4g\\t\\t15\\tpsbattle_artwork\\teveryone looks so yummy\\t\\t0\\t2\\t4\\n\\tthis guy at the adult vr festival in tokyo this weekend\\t1465819487.0\\ti.kinja-img.com\\tTrue\\t4nvas9\\thttps://external-preview.redd.it/1XEjZYusUBXwJndja1lHPuCW4NFz7j80OCfBd7XiTAY.jpg?width=320&crop=smart&auto=webp&s=e80adfcb9b709fb98556ce09261ee53de045800b\\t\\t349.0\\t9551\\tphotoshopbattles\\tPsBattle: This guy at the Adult VR Festival in Tokyo this weekend\\t0.89\\t1\\t0\\t0\\nDizzyEllie\\tobligitory\\t1476226457.0\\t\\tTrue\\td8o0chd\\thttp://i.imgur.com/z0PnFE0.jpg\\t56z02p\\t\\t2\\tpsbattle_artwork\\tObligitory.\\t\\t0\\t2\\t4\\nall-top-today_SS\\telephant seal pup says hello to a post from a few floors above\\t1559336297.0\\ti.redd.it\\tTrue\\tbvctt3\\thttps://preview.redd.it/g0c53pmrlf131.jpg?width=320&crop=smart&auto=webp&s=1de627c4a23e85b629d107d7e1ebc7b06b1d39ee\\t\\t18.0\\t101\\tsubredditsimulator\\tElephant seal pup says hello to a post from a few floors above\\t0.96\\t0\\t2\\t3\\nshinji3\\twhat elon doesnt want you to see\\t1518287809.0\\t\\tTrue\\tdu1mjqa\\thttps://i.imgur.com/xRbpvxW.jpg\\t7wl1vo\\t\\t71\\tpsbattle_artwork\\tWhat Elon doesn\\'t want you to see\\t\\t0\\t2\\t4\\nTheCreativeNick\\tblue turtle with a dirty shell\\t1563833741.0\\ti.redd.it\\tTrue\\tcgjwhn\\thttps://preview.redd.it/9qdsw151jxb31.jpg?width=320&crop=smart&auto=webp&s=c4c5c2ade1810433a3e371e72a5a2d848464f7ae\\t\\t408.0\\t24511\\tphotoshopbattles\\tPsBattle: Blue turtle with a dirty shell\\t0.96\\t1\\t0\\t0\\ncauseofb\\tjapanese depiction of an american warship\\t1476659895.0\\ti.imgur.com\\tTrue\\t57u8n9\\thttps://external-preview.redd.it/DaZZGGLadbj-qSzlJ96STTAfhrlUqz6MbTn5AK3wsGw.jpg?width=320&crop=smart&auto=webp&s=d188c0f83b3d2e8072ad9c7bf585cbb682876baa\\t\\t67.0\\t1105\\tpropagandaposters\\tJapanese depiction of an American warship, 1854\\t0.97\\t0\\t1\\t5\\n\\tlittle girl in a white dress looking out a window\\t1436536622.0\\timgur.com\\tTrue\\t3csr8n\\thttps://external-preview.redd.it/PwEtMoKSLV6YSSn3kpk-UyRocldOPcAEAGqQO9XoO2k.jpg?width=320&crop=smart&auto=webp&s=87f9ff5781ff0b8153faba6f7387211fa953b0bf\\t\\t6.0\\t100\\tmisleadingthumbnails\\tLittle girl in a white dress looking out a window.\\t0.86\\t0\\t2\\t2\\nDrXtreme28\\tkeep your head above\\t1399860594.0\\t\\tTrue\\tchfm9uw\\thttp://i.imgur.com/J4Qwfi4.jpg\\t25b9e2\\t\\t31\\tpsbattle_artwork\\tKeep your head above\\t\\t0\\t2\\t4\\nIf_You_Only_Knew\\toh north koreas invading well let me just go and get my fucking clown shoes\\t1363142770.0\\t\\tTrue\\tc8umnyd\\thttp://i.imgur.com/HMPT52I.jpg\\t1a6upj\\t\\t27\\tpsbattle_artwork\\tOh, North Korea\\'s invading? Well, let me just go and get my fucking clown shoes.\\t\\t0\\t2\\t4\\nGetMotivated_SS\\tarticle one of the rest of us are wasting our lives\\t1448287091.0\\tbrainpickings.org\\tTrue\\t3txxj1\\thttps://external-preview.redd.it/r2qrGFfHPGuFWTCeXdjcly6Ew7UlC5yR2urSZuHoW74.jpg?width=320&crop=smart&auto=webp&s=381feada4235ba6e4686f149b41f78007bd7a02e\\t\\t20.0\\t9\\tsubredditsimulator\\t[Article] One of the rest of us are wasting our lives\\t0.91\\t0\\t2\\t3\\nartunitinc\\tthis one\\t1468845225.0\\t\\tTrue\\td5gpz50\\thttp://i.imgur.com/JPJBsz0.jpg\\t4td1bc\\t\\t33\\tpsbattle_artwork\\tthis one?\\t\\t0\\t2\\t4\\nkoi666\\tbored christmas cat\\t1482768667.0\\ti.reddituploads.com\\tTrue\\t5kecl5\\thttps://external-preview.redd.it/_oeEZ21UIWYl9qySzhuhOBihXfL8pHd-q4ThRcoTuTI.jpg?width=320&crop=smart&auto=webp&s=3f81abc8b3013db4e2121338709f2242c20abdcb\\t\\t1.0\\t5\\tphotoshopbattles\\tPsBattle: Bored Christmas cat\\t0.86\\t1\\t0\\t0\\nkluste\\twhere is the fucking backup\\t1476130417.0\\t\\tTrue\\td8mbzus\\thttp://i.imgur.com/4BkCTlU.jpg\\t56tith\\t\\t2\\tpsbattle_artwork\\tWhere is the fucking backup!?!?!\\t\\t0\\t2\\t4\\nal0c-ac0c\\tbeen staring at this one eared cyclops mouse on my train ride through scotland\\t1568824896.0\\timgur.com\\tTrue\\td60g9w\\thttps://external-preview.redd.it/Y3Wr32rXNPudybPwgaFelgcf2rkiyrGSb9jmV8n0nCA.jpg?width=320&crop=smart&auto=webp&s=f99b7f97505aadb40d816857877158668a3b85b5\\t\\t1.0\\t6\\tpareidolia\\tBeen staring at this one eared Cyclops mouse on my train ride through Scotland.\\t0.88\\t0\\t2\\t2\\nall-top-today_SS\\tmy first gaming pc just arrived in brazil i am sick of politicians giving the same thing as donald trumps star on the head\\t1485817095.0\\ti.imgur.com\\tTrue\\t5r48ow\\thttps://external-preview.redd.it/ESPM-tTuK3werLgfWG_wBsw_trUSDNU4bI5s2jVRAgQ.png?width=108&crop=smart&auto=webp&s=6bdf4d9c8ec6b3100f803c1cf0848f49a5aee344\\t\\t20.0\\t31\\tsubredditsimulator\\tMy first gaming pc just arrived in Brazil, I am sick of politicians giving the same thing as Donald Trump\\'s star on the head\\t0.88\\t0\\t2\\t3\\nalsobrante\\tmit engineer browsing incognito mode on early computer\\t1538338579.0\\ti.imgur.com\\tTrue\\t9k9lgj\\thttps://external-preview.redd.it/O20cyX0xei4nvxlKlFepHyl3JcqI3Hm-pN7E2415_U8.png?width=320&crop=smart&auto=webp&s=d16e7d0d23250c36644e90b66e2e5c9fe559dd6d\\t\\t4.0\\t173\\tfakehistoryporn\\tMIT engineer browsing incognito mode on early computer (1983)\\t1.0\\t0\\t2\\t2\\nalloutnow\\ttrump blocks release of democratic memo on russia probe\\t1518225144.0\\treuters.com\\tTrue\\t7wi82s\\thttps://external-preview.redd.it/U5Y07YbILjhjvhQiiaawsr2jdIwnO2Fxd1C_d24xYw4.jpg?width=320&crop=smart&auto=webp&s=9edaf07bda93aa9a9d24ee418fa53470fbd32cc2\\t\\t1.0\\t23\\tusanews\\tTrump blocks release of Democratic memo on Russia probe | Reuters\\t0.83\\t1\\t0\\t0\\nFoxprowl\\tthe city of los angeles at night seen from space\\t1502384619.0\\ti.redd.it\\tTrue\\t6sup0y\\thttps://preview.redd.it/w3y6f99jixez.jpg?width=320&crop=smart&auto=webp&s=69fa24f2862da454d01a71b6639a27ad491fcbc6\\t\\t0.0\\t10\\tmisleadingthumbnails\\tThe city of Los Angeles at night seen from space\\t0.71\\t0\\t2\\t2\\nblackbanhmi\\ta coffee shop in vietnam where you can soak your feet in anklelevel water with swimming fish\\t1537851239.0\\ti.redd.it\\tTrue\\t9ip9m3\\thttps://preview.redd.it/y1grvezrfbo11.jpg?width=320&crop=smart&auto=webp&s=85a8240a4042389f1d53d9046f983993ddabebd8\\t\\t24.0\\t87\\tmildlyinteresting\\tA coffee shop in Vietnam, where you can soak your feet in ankle-level water with swimming fish\\t0.91\\t1\\t0\\t0\\nSimmo5150\\tthe strangers at the mountains of madness\\t1522416520.0\\ti.redd.it\\tTrue\\t88abtq\\thttps://preview.redd.it/f5vy9qi7kwo01.jpg?width=320&crop=smart&auto=webp&s=b819fa1c7096cf275cf65a26dca2e33b64e429aa\\t\\t2.0\\t49\\tfakealbumcovers\\tThe Strangers - At The Mountains Of Madness\\t0.98\\t0\\t2\\t1\\n\\tirafsky district tender compassions x\\t1422020514.0\\ti.imgur.com\\tTrue\\t2tehcz\\thttps://external-preview.redd.it/FZyyQnZrEkI997h31MdaUxrkQf4hlDirrd9uLXIckOc.png?width=320&crop=smart&auto=webp&s=c71ff486b5d749cd93313a3cc14d86cbeae3e2e2\\t\\t1.0\\t7\\tfakealbumcovers\\tIrafsky District - Tender Compassions {1040x1040}\\t1.0\\t0\\t2\\t1\\nroset_ta\\tthis world leader missing from his chair\\t1566852595.0\\ti.redd.it\\tTrue\\tcvtwtq\\thttps://preview.redd.it/poaq69oe7ti31.jpg?width=320&crop=smart&auto=webp&s=ad1bb72a6d731a440bcd33256c53b1504c1540a0\\t\\t14.0\\t30\\tphotoshopbattles\\tPsBattle: This world leader missing from his chair.\\t0.81\\t1\\t0\\t0\\nOxTox\\tmy cat dreaming about his future as a basketballer\\t1374694335.0\\ti.imgur.com\\tTrue\\t1iz5p0\\thttps://external-preview.redd.it/Kt6FWE0rz41rDzlHbrtZjl1nZl4SBnTOBquEJ7DHbyQ.jpg?width=320&crop=smart&auto=webp&s=d69173783daa7e0dba2c9d2b3abfdadc650d5d69\\t\\t6.0\\t3\\tphotoshopbattles\\tMy cat dreaming about his future as a basketballer.\\t0.72\\t1\\t0\\t0\\nChesleaFc\\ther period gone wild\\t1420259337.0\\ti.imgur.com\\tTrue\\t2r69ma\\thttps://external-preview.redd.it/psWSM7m7r_Zuw7dMUg_BBi4BLMor_CRXtEZkrcFyLww.jpg?width=320&crop=smart&auto=webp&s=fb315ebd5bdae391657e48b8fad4bdebe63b8af3\\t\\t5.0\\t63\\tmisleadingthumbnails\\tHer period gone wild\\t0.68\\t0\\t2\\t2\\nMyEmptyBagOfChips\\tthese snails crowded on a tree\\t1557935912.0\\ti.imgur.com\\tTrue\\tbozp05\\thttps://external-preview.redd.it/6O_jT6jvg3Hw1_yIeiJuo1c621jDd2cJYCQcb2JNkmo.jpg?width=320&crop=smart&auto=webp&s=c8acfe313a1737754086f16f07a30124da959b6a\\t\\t5.0\\t21\\tphotoshopbattles\\tPsBattle: These snails crowded on a tree\\t0.8\\t1\\t0\\t0\\nfried_egg_on_toast\\ti was driving and saw this house that had a sperm kite flying outside\\t1564947960.0\\ti.redd.it\\tTrue\\tcm0o0a\\thttps://preview.redd.it/krgpibi8khe31.jpg?width=320&crop=smart&auto=webp&s=18fb1c436e2a825af71a2380646a29297d51affc\\t\\t14.0\\t8\\tmildlyinteresting\\tI was driving and saw this house that had a sperm kite flying outside\\t0.79\\t1\\t0\\t0\\nramong941\\tdont skip leg day\\t1447339550.0\\ti.imgur.com\\tTrue\\t3sjdx0\\thttps://external-preview.redd.it/Y-UA9ez9xIbJ7DW4ee44psznfbgrZYDlp9MCd_aNS6M.jpg?width=320&crop=smart&auto=webp&s=e2fbd201c861263735f76e0ad196907448736152\\t\\t3.0\\t366\\tconfusing_perspective\\tDon\\'t skip leg day\\t0.95\\t0\\t2\\t2\\nApiContraption\\tcutouts\\t1407988458.0\\t\\tTrue\\tcjpqh64\\thttp://i.imgur.com/aQdK3Cd%2ejpg\\t2di8gm\\t\\t1\\tpsbattle_artwork\\tcutouts\\t\\t0\\t2\\t4\\nJohnJTaylor\\tstate has st elk sighting in more than centuries\\t1477570409.0\\tmsn.com\\tTrue\\t59nmlp\\thttps://external-preview.redd.it/GK9zzWdnSgLiJfA0UoZ-6RI4PgInibREyfSNBj_xZHo.jpg?width=320&crop=smart&auto=webp&s=31042c360e26d9aea8294e62ff4ea3636b5ff12f\\t\\t3.0\\t18\\tupliftingnews\\tState has 1st elk sighting in more than 2 centuries\\t0.89\\t1\\t0\\t0\\nWOSBen\\twish you were here\\t1468486664.0\\t\\tTrue\\td5bqhmr\\thttp://i.imgur.com/6dHjTDU.jpg\\t4srok6\\t\\t550\\tpsbattle_artwork\\t\"Wish you were here\"\"\"'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#first line after cleaning\n",
        "cleantext(df.iloc[0][\"text\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "id": "zo4lWA2bbRXV",
        "outputId": "fabc3fa0-9b3e-42d5-ebed-c9a5ac69cd82"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'group friend began volunteer homeless shelter neighbor protested seeing another person also need naturally like want help person neomegaryumkii denmark trolled trump bus watch happens move archive true cu link savedyouaclick denmark trolled trump bus watch happens move back wheel look like eye spin crazily bus move saved click short video go slow motion video mirror comment since archive removed atbthefirst young man there need feel true cqiw link ute psbattle artwork young man need feel beosronlto onion ring chimp imgur com true s link pareidolia onion ring chimp food s eat image description imgur com true mnjdi link subredditsimulator eat image description cardboardsoyuz arizona man purchase home discovers underground hatch backyard web archive org true zmas link savedyouaclick arizona man purchase home discovers underground hatch backyard lead old fall shelter pretty good shape couple can supply click harry butz rpics award spell penis redd true cpajkp link mildlyinteresting pic award spell penis billarastg time traveller assassin attemps take obama becomes president circa redd true awrrxk link fakehistoryporn time traveller assassin attemps take obama becomes president circa missmaria ew true cjhqf link coipv psbattle artwork ew snlisha islamic state fighter reenter ancient palmyra syria bbc com true hlch link neutralnews islamic state fighter enter ancient palmyra syria raidyrafy top golf red line warning sign many people gotten hurt redd true vcq link mildlyinteresting top golf red line warning sign many people gotten hurt budrickbundy nfls rating slide mainly fueled white viewer younger viewer awfulannouncing com true qt fc link usanews nfl rating slide mainly fueled white viewer younger viewer bbizzess meryl streep marries archive true gen link savedyouaclick meryl streep marries article say marrying robert redford apicontraption discussion true cfqkht link psbattle artwork discussion askesl squash dingres true cx jt link tfgjo psbattle artwork squash ingres grahamsaysno cactus man imgur com true ev link pareidolia cactus man ishitpoopsalot roommate stab debate iphone samsung ktul com true wpnv link nottheonion roommate stab debate iphone samsung slithek got rid ant true e hx link cairr psbattle artwork got rid ant jj titanic true emv vw link bm yhe psbattle artwork titanic thigira declaration independence philadelphia redd true wf link fakehistoryporn declaration independence philadelphia colorized bobblee cloud formation sunrise look like fire tornado redd true au fsm link mildlyinteresting cloud formation sunrise look like fire tornado chispy obligatory faceswap true cdmk link zzhjk psbattle artwork obligatory faceswap vt excited dog imgur com true dekok link photoshopbattles excited dog ratchetraccoon photograph first interracial family pennsylvania interracial marriage legalized circa redd true ku link fakehistoryporn colorized photograph first interracial family pennsylvania interracial marriage legalized circa mrtenpence local bus usb charger every seat imgur com true iri link mildlyinteresting local bus usb charger every seat lucastwaddle fact people keep buying stuff ace harware work texas redd true link mildlyinteresting fact people keep buying stuff ace harware work texas originalstix huge tennis racket mini federer true cymkq link zfune psbattle artwork huge tennis racket mini federer mangobutter cigs mexico photo dead body also picked pharmacy selling secretly lol redd true link mildlyinteresting cigs mexico photo dead body also picked pharmacy selling secretly lol elefanttwoothpaste fat man bomb dropped nagasaki imgur com true cz link misleadingthumbnails fat man bomb dropped nagasaki colorized viperdream death close true mws link biplz psbattle artwork death close mohhh british soldier last stand battle el alamein redd true fx link fakehistoryporn british soldier last stand battle el alamein colorized wafflesmoof year old doingthis imgur com true pyfx link photoshopbattles psbattle year old wlms boov true dafx link etd psbattle artwork boov lordmetrognome puppy redd true tmcc link photoshopbattles psbattle puppy swigsweg raced another plane flight back europe day imgur com true ey ng link mildlyinteresting raced another plane flight back europe day blackjack texas woman us coupon clipping skill help hurricane survivor abcnews go com true na link upliftingnews texas woman us coupon clipping skill help hurricane survivor apicontraption cutout true ckzeba link ymd psbattle artwork cutout jargos idk might nsfw true caxeexm link h jh psbattle artwork idk might nsfw ene due rabe deep inside shes different true dxs link py psbattle artwork deep inside different rizilus big head true dxv mcd link ee tt psbattle artwork big head chibucks everyone look yummy true cwdtq link psbattle artwork everyone look yummy guy adult vr festival tokyo weekend kinja img com true nvas link photoshopbattles psbattle guy adult vr festival tokyo weekend dizzyellie obligitory true chd link psbattle artwork obligitory top today s elephant seal pup say hello post floor redd true bvctt link subredditsimulator elephant seal pup say hello post floor shinji elon doesnt want see true du mjqa link wl vo psbattle artwork elon want see thecreativenick blue turtle dirty shell redd true cgjwhn link photoshopbattles psbattle blue turtle dirty shell causeofb japanese depiction american warship imgur com true link propagandaposters japanese depiction american warship little girl white dress looking window imgur com true csr link misleadingthumbnails little girl white dress looking window drxtreme keep head true chfm uw link psbattle artwork keep head knew oh north korea invading well let go get fucking clown shoe true umnyd link upj psbattle artwork oh north korea invading well let go get fucking clown shoe getmotivated s article one rest u wasting life brainpickings org true txxj link subredditsimulator article one rest u wasting life artunitinc one true gpz link td bc psbattle artwork one koi bored christmas cat reddituploads com true kecl link photoshopbattles psbattle bored christmas cat kluste fucking backup true mbzus link tith psbattle artwork fucking backup al ac staring one eared cyclops mouse train ride scotland imgur com true link pareidolia staring one eared cyclops mouse train ride scotland top today s first gaming pc arrived brazil sick politician giving thing donald trump star head imgur com true ow link subredditsimulator first gaming pc arrived brazil sick politician giving thing donald trump star head alsobrante mit engineer browsing incognito mode early computer imgur com true lgj link fakehistoryporn mit engineer browsing incognito mode early computer alloutnow trump block release democratic memo russia probe reuters com true wi link usanews trump block release democratic memo russia probe reuters foxprowl city los angeles night seen space redd true sup link misleadingthumbnails city los angeles night seen space blackbanhmi coffee shop vietnam soak foot anklelevel water swimming fish redd true ip link mildlyinteresting coffee shop vietnam soak foot ankle level water swimming fish simmo stranger mountain madness redd true abtq link fakealbumcovers stranger mountain madness irafsky district tender compassion imgur com true tehcz link fakealbumcovers irafsky district tender compassion roset ta world leader missing chair redd true cvtwtq link photoshopbattles psbattle world leader missing chair oxtox cat dreaming future basketballer imgur com true iz link photoshopbattles cat dreaming future basketballer chesleafc period gone wild imgur com true link misleadingthumbnails period gone wild myemptybagofchips snail crowded tree imgur com true bozp link photoshopbattles psbattle snail crowded tree fried egg toast driving saw house sperm kite flying outside redd true cm link mildlyinteresting driving saw house sperm kite flying outside ramong dont skip leg day imgur com true sjdx link confusing perspective skip leg day apicontraption cutout true cjpqh link di gm psbattle artwork cutout johnjtaylor state st elk sighting century msn com true nmlp link upliftingnews state st elk sighting century wosben wish true bqhmr link srok psbattle artwork wish'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# clean all text"
      ],
      "metadata": {
        "id": "kXSURBtzJkaN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df[\"text\"] = df[\"text\"].apply(cleantext)"
      ],
      "metadata": {
        "id": "qMK-_uqxbfVb"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 10 most freq text"
      ],
      "metadata": {
        "id": "H5NWEiYn6sT9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "frquency_ofwords = pd.Series(\" \".join(df[\"text\"]).split()).value_counts()\n",
        "frquency_ofwords[0:10]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OonC-l57cIaz",
        "outputId": "fa3b29ac-7559-4eab-c813-006bf52118e2"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "year         4124\n",
              "one          3285\n",
              "new          2998\n",
              "like         2949\n",
              "man          2706\n",
              "trump        2577\n",
              "colorized    2430\n",
              "people       2314\n",
              "first        2247\n",
              "old          2221\n",
              "Name: count, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 10 least freq text"
      ],
      "metadata": {
        "id": "WAJmjWmNJxLI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "frquency_ofwords[-10:]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "luEAUl1beJo3",
        "outputId": "1680fe30-66b7-44c3-b39b-19e736d7a451"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "villers       1\n",
              "wittmann      1\n",
              "monotonous    1\n",
              "ingi          1\n",
              "zdko          1\n",
              "chuoc         1\n",
              "bui           1\n",
              "laurent       1\n",
              "motha         1\n",
              "wahre         1\n",
              "Name: count, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "analyzer=\"word\": Specifies that the feature should be made of word n-grams.\n",
        "\n",
        "max_df=0.3: Ignores terms that have a document frequency strictly higher than the given threshold (in this case, 0.3, which means terms appearing in more than 30% of the documents will be ignored).\n",
        "\n",
        "min_df=10: Ignores terms that have a document frequency strictly lower than the given threshold (in this case, 10, which means terms appearing in less than 10 documents will be ignored).\n",
        "\n",
        "ngram_range=(1, 2): Specifies that the feature should be made of unigrams and bigrams.\n",
        "\n",
        "norm=\"l2\": Applies L2 normalization to the TF-IDF matrix, which means each output row will have unit norm."
      ],
      "metadata": {
        "id": "x_IRviAIhKqi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer  # Importing TfidfVectorizer from scikit-learn\n",
        "\n",
        "# Initializing TfidfVectorizer with specified parameters\n",
        "vectorTFIDF = TfidfVectorizer(\n",
        "    analyzer=\"word\",  # Specify word-level analyzer\n",
        "    max_df=0.3,  # Ignore terms that appear in more than 30% of the documents\n",
        "    min_df=10,  # Ignore terms that appear in less than 10 documents\n",
        "    ngram_range=(1, 2),  # Consider both unigrams and bigrams\n",
        "    norm=\"l2\"  # Apply L2 normalization to vectorized output\n",
        ")\n",
        "\n",
        "# Fit the TfidfVectorizer on the text data in the DataFrame column \"text\"\n",
        "vectorTFIDF.fit(df[\"text\"])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "N8U88ABEeS-_",
        "outputId": "0ff33ce8-0ab5-45c5-d13a-e2727d035e2d"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TfidfVectorizer(max_df=0.3, min_df=10, ngram_range=(1, 2))"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"‚ñ∏\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"‚ñæ\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>TfidfVectorizer(max_df=0.3, min_df=10, ngram_range=(1, 2))</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer(max_df=0.3, min_df=10, ngram_range=(1, 2))</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sampling 5 random ngrams (words or word sequences) from the vocabulary generated by the TfidfVectorizer\n",
        "Randomngram = pd.Series(vectorTFIDF.vocabulary_).sample(5, random_state=1)\n",
        "\n",
        "# Printing the sampled ngrams to the console\n",
        "print(f\"Unique word (ngram) vector extract:\\n {Randomngram}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5iOPgyTdha4D",
        "outputId": "fe9917d9-5f9e-42ae-88ce-851355811f29"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique word (ngram) vector extract:\n",
            " baby             707\n",
            "remains         8171\n",
            "homeless man    4579\n",
            "different       2644\n",
            "explains        3306\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "dividing the train dataset to train and validation"
      ],
      "metadata": {
        "id": "2BSrrjTimmt7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X = df[\"text\"]\n",
        "y = df[\"label\"]\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, random_state=1, test_size=0.20, shuffle=True, stratify=y)"
      ],
      "metadata": {
        "id": "sRPI2CsfhmIz"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train.shape)\n",
        "print(X_val.shape)\n",
        "print(y_train.shape)\n",
        "print(y_val.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c_toAy_Kh8dk",
        "outputId": "7ba883e9-4d69-4b60-8791-12216f78a687"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(47814,)\n",
            "(11954,)\n",
            "(47814,)\n",
            "(11954,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this code snippet, a pipeline is created using scikit-learn's Pipeline class. The pipeline consists of two steps: TfidfVectorizer and XGBoost classifier. TfidfVectorizer is configured with ngram_range=(1, 3) and analyzer='word'. The XGBoost classifier is used with default hyperparameters. Additionally, a parameter grid (param_xgb) is defined to specify the hyperparameter search space for grid search or randomized search. The parameter grid includes hyperparameters for both TfidfVectorizer and XGBoost classifier. This pipeline can be used for text classification tasks where text data needs to be transformed into TF-IDF vectors before being fed into an XGBoost model."
      ],
      "metadata": {
        "id": "XtZ7dI-tMbAy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import Pipeline  # Importing Pipeline from scikit-learn\n",
        "from xgboost.sklearn import XGBClassifier  # Importing XGBClassifier from XGBoost\n",
        "\n",
        "# Defining the hyperparameter grid for XGBoost classifier and TfidfVectorizer\n",
        "param_xgb = {\n",
        "#     \"tfidf__analyzer\":[\"word\",\"char\"],\n",
        "#     \"tfidf__ngram_range\": [(1, 2)],\n",
        "    \"tfidf__max_df\": np.arange(0.2, 0.8),  # Range of max_df values for TfidfVectorizer\n",
        "    \"tfidf__min_df\": np.arange(5, 20),  # Range of min_df values for TfidfVectorizer\n",
        "    'my_xgb__gamma': [0.5, 1, 1.5, 2, 5],  # Values for the gamma parameter in XGBoost\n",
        "    'my_xgb__subsample': [0.6, 0.8, 1.0],  # Values for the subsample parameter in XGBoost\n",
        "    'my_xgb__colsample_bytree': [0.6, 0.8, 1.0],  # Values for colsample_bytree in XGBoost\n",
        "    'my_xgb__max_depth': [3, 4, 5]  # Values for the max_depth parameter in XGBoost\n",
        "}\n",
        "\n",
        "# Creating a pipeline with TfidfVectorizer and XGBoost classifier\n",
        "xgb_pipeline = Pipeline(\n",
        "    steps=[\n",
        "        (\"tfidf\", TfidfVectorizer(ngram_range=(1, 3), analyzer='word')),  # Adding TfidfVectorizer to the pipeline\n",
        "        ('my_xgb', XGBClassifier())  # Adding XGBoost classifier to the pipeline\n",
        "    ]\n",
        ")\n"
      ],
      "metadata": {
        "id": "BzKWPItch9Pa"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this code snippet, a pipeline is created using scikit-learn's Pipeline class. The pipeline consists of two steps: TfidfVectorizer and RandomForestClassifier. TfidfVectorizer is configured with ngram_range=(1, 2), analyzer='word', max_df=0.85, min_df=2, and norm=\"l2\". RandomForestClassifier is used with n_estimators=126, specifying the number of decision trees in the forest. This pipeline can be used for text classification tasks where text data needs to be transformed into TF-IDF vectors before being fed into a RandomForestClassifier model."
      ],
      "metadata": {
        "id": "Vb9o-rnrMnSB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier  # Importing RandomForestClassifier from scikit-learn\n",
        "\n",
        "# Creating a pipeline with TfidfVectorizer and RandomForestClassifier\n",
        "RF_pipeline = Pipeline(\n",
        "    steps=[\n",
        "        (\"tfidf\", TfidfVectorizer(ngram_range=(1, 2), analyzer='word', max_df=0.85, min_df=2, norm=\"l2\")),  # Adding TfidfVectorizer to the pipeline\n",
        "        ('RF', RandomForestClassifier(n_estimators=126))  # Adding RandomForestClassifier to the pipeline\n",
        "    ]\n",
        ")\n"
      ],
      "metadata": {
        "id": "VlUwHz1liO4X"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this code snippet, a predefined split is created using scikit-learn's PredefinedSplit class. The predefined split is useful when you have a specific training and validation set that you want to use for cross-validation. The variable split_index is created to specify the fold (training or validation) each sample belongs to. A value of -1 indicates that the sample belongs to the training set, while a value of 0 indicates that the sample belongs to the validation set. Finally, a PredefinedSplit object pds is created with the test fold indices provided by split_index. This predefined split can then be used in cross-validation procedures to ensure that the training and validation sets are kept consistent across different folds."
      ],
      "metadata": {
        "id": "Fr8fq0CPMyqU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import PredefinedSplit  # Importing PredefinedSplit from scikit-learn\n",
        "\n",
        "# Creating a predefined split for cross-validation\n",
        "split_index = [-1 if x in X_train.index else 0 for x in df.index]  # Creating an index array for predefined split\n",
        "pds = PredefinedSplit(test_fold=split_index)  # Creating a PredefinedSplit object with test fold indices\n"
      ],
      "metadata": {
        "id": "7_mcuXwAjByD"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# trial 1\n",
        "\n",
        "In Trial 1, a pipeline with TfidfVectorizer and XGBoost classifier was created. TfidfVectorizer was configured with an ngram_range of (1, 3) and an analyzer set to 'word'. The pipeline was then fitted to the training data (`X_train` and `y_train`), and predictions were made on the validation data (`X_val`). The performance of the model was evaluated using the ROC AUC score.\n",
        "\n",
        "The obtained ROC AUC score of 0.7235 indicates the model's ability to distinguish between positive and negative classes, with a value closer to 1 indicating better performance.\n",
        "\n",
        "Comparing the obtained ROC AUC score of 0.7235 with the parameters chosen in Trial 1, we can assess the effectiveness of the model configuration:\n",
        "\n",
        "1. **TfidfVectorizer Parameters:**\n",
        "   - `ngram_range=(1, 3)`: This parameter defines the range of n-grams to consider. In Trial 1, it was set to (1, 3), meaning both unigrams, bigrams, and trigrams were included in the feature representation. This allows the model to capture both single words and sequences of up to three words.\n",
        "   - `analyzer='word'`: This parameter specifies whether the feature should be made of word n-grams (when set to 'word') or character n-grams (when set to 'char'). In Trial 1, it was set to 'word', indicating that the TfidfVectorizer should consider words as features.\n",
        "\n",
        "2. **XGBoost Classifier Parameters:**\n",
        "   - No specific parameters were utilized for XGBoost classifier in Trial 1. Therefore, the default hyperparameters of XGBoost were used.\n",
        "\n",
        "**Analysis:**\n",
        "- The ROC AUC score of 0.7235 suggests that the chosen parameters were reasonably effective in distinguishing between fake and genuine news posts based on their titles.\n",
        "- The inclusion of both unigrams, bigrams, and trigrams in the feature representation allowed the model to capture both individual words and phrases, potentially improving its predictive performance.\n",
        "- Setting the analyzer parameter to 'word' ensured that words were considered as features, which is appropriate for this text classification task.\n",
        "\n",
        "**Further Steps:**\n",
        "- Despite achieving a decent ROC AUC score, further experimentation with hyperparameters and feature engineering techniques may help in improving the model's performance.\n",
        "- Exploring different combinations of hyperparameters for both TfidfVectorizer and XGBoost classifier could lead to better results.\n",
        "- Additionally, fine-tuning the model using techniques such as cross-validation may further enhance its predictive capabilities."
      ],
      "metadata": {
        "id": "hKmAEiVHM5tu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "xgb_pipeline.fit(X_train,y_train)\n",
        "y_pred = xgb_pipeline.predict(X_val)\n",
        "acc = roc_auc_score(y_pred, y_val)\n",
        "print(acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YnKagtW0_LxG",
        "outputId": "adaab587-03db-4e4b-e2f2-540f01c4554d"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.723521145302408\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# trial 2\n",
        "\n",
        "Comparing the obtained ROC AUC score of 0.7646 in Trial 2 with the parameters chosen reveals the effectiveness of the model configuration:\n",
        "\n",
        "1. **TfidfVectorizer Parameters:**\n",
        "   - `ngram_range=(1, 2)`: Unlike Trial 1, Trial 2 utilized only unigrams and bigrams (sequences of up to two words) in the feature representation. This narrower range may have resulted in a more focused representation of the text data.\n",
        "   - `analyzer='word'`: Similar to Trial 1, the TfidfVectorizer was configured to consider words as features.\n",
        "\n",
        "2. **Random Forest Classifier Parameters:**\n",
        "   - `n_estimators=126`: This parameter determines the number of decision trees in the random forest. In Trial 2, 126 decision trees were used. A larger number of trees may lead to better model performance, albeit at the cost of increased computational complexity.\n",
        "\n",
        "**Analysis:**\n",
        "- The ROC AUC score of 0.7646 in Trial 2 indicates improved performance compared to Trial 1 (0.7235).\n",
        "- Limiting the ngram_range to (1, 2) may have helped focus the model's attention on more informative features, leading to better discrimination between fake and genuine news posts.\n",
        "- The use of RandomForestClassifier instead of XGBoost in Trial 2 may have also contributed to the improvement in performance. Random forests are known for their robustness and ability to handle big data effectively.\n",
        "\n",
        "**Comparison with Trial 1:**\n",
        "- Trial 2 achieved a higher ROC AUC score (0.7646) compared to Trial 1 (0.7235), indicating that the model in Trial 2 was more successful in distinguishing between fake and genuine news posts.\n",
        "- The differences in parameter settings, including the choice of ngram_range and classifier, likely contributed to the variation in performance between the two trials.\n",
        "\n"
      ],
      "metadata": {
        "id": "V7eG7GyXM8eu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "RF_pipeline.fit(X_train,y_train)\n",
        "y_pred = RF_pipeline.predict(X_val)\n",
        "acc = roc_auc_score(y_pred, y_val)\n",
        "print(acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m0P7Cn1jjH1n",
        "outputId": "d02e55ba-d7e4-45db-f994-704777ad37eb"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7645863054223027\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# trial 3\n",
        "\n",
        "Comparing the obtained ROC AUC score of 0.6418 in Trial 3 with the defined parameters, as well as comparing it with Trials 1 and 2,\n",
        "\n",
        "1. **TfidfVectorizer Parameters:**\n",
        "   - `analyzer='char'`: In Trial 3, the TfidfVectorizer was configured to use character-level analysis instead of word-level analysis. This means that the model focuses on character sequences rather than words.\n",
        "   - `ngram_range=(1, 2)`: Despite specifying a ngram_range of (1, 2), character-level analysis would still consider sequences of characters, not words. Therefore, this parameter setting doesn't have the same interpretation as in Trials 1 and 2.\n",
        "   - `max_df`, `min_df`: The values of max_df and min_df were explored within the ranges of 0.2 to 0.8 and 5 to 20, respectively, allowing for different thresholds of document frequency for feature selection.\n",
        "\n",
        "2. **XGBoost Classifier Parameters:**\n",
        "   - Hyperparameters such as `gamma`, `subsample`, `colsample_bytree`, and `max_depth` were tuned within specific ranges to optimize the XGBoost classifier's performance.\n",
        "\n",
        "**Analysis:**\n",
        "- The ROC AUC score of 0.6418 in Trial 3 is lower compared to Trials 1 and 2, indicating that the model's performance was not as effective in distinguishing between fake and genuine news posts.\n",
        "- Utilizing character-level analysis instead of word-level analysis may have resulted in a loss of semantic information, making it more challenging for the model to extract meaningful features from the text data.\n",
        "- The chosen hyperparameters for the XGBoost classifier may not have been optimal for this particular dataset and task, leading to suboptimal performance.\n",
        "\n",
        "**Comparison with Trials 1 and 2:**\n",
        "- Trials 1 and 2 achieved higher ROC AUC scores (0.7235 and 0.7646, respectively) compared to Trial 3 (0.6418), indicating that the models in Trials 1 and 2 were more successful in distinguishing between fake and genuine news posts.\n",
        "- The differences in parameter settings, including the choice of analyzer and hyperparameters, likely contributed to the variation in performance between the trials.\n"
      ],
      "metadata": {
        "id": "dTvWJ1nZNAbA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Define the parameter grid for hyperparameter tuning\n",
        "param_xgb = {\n",
        "    \"tfidf__analyzer\":[\"char\"],\n",
        "    \"tfidf__ngram_range\": [(1, 2)],\n",
        "    \"tfidf__max_df\": np.arange(0.2, 0.8),\n",
        "    \"tfidf__min_df\": np.arange(5, 20),\n",
        "    'my_xgb__gamma': [ 0.5],\n",
        "    'my_xgb__subsample': [0.6],\n",
        "    'my_xgb__colsample_bytree': [0.6],\n",
        "    'my_xgb__max_depth': [3]\n",
        "\n",
        "}\n",
        "\n",
        "# Create GridSearchCV object with the defined parameter grid\n",
        "grid_search1 = GridSearchCV(xgb_pipeline, param_xgb, cv=2, verbose=1, n_jobs=2, scoring='roc_auc')\n",
        "\n",
        "# Fit the GridSearchCV object to the data\n",
        "grid_search1.fit(X_train,y_train)\n",
        "\n",
        "# Predict probabilities using the best estimator found by GridSearchCV\n",
        "y_pred_proba = grid_search1.predict(X_val)\n",
        "\n",
        "# Print the ROC AUC score using the true labels and predicted probabilities\n",
        "print(roc_auc_score(y_val, y_pred_proba))\n",
        "\n",
        "# Print the best parameters found by GridSearchCV\n",
        "print('best params: {}'.format(grid_search1.best_params_))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vw3Im14P_kqr",
        "outputId": "26063d36-4eaf-410d-ea75-f93770025e54"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 2 folds for each of 15 candidates, totalling 30 fits\n",
            "0.641813186712625\n",
            "best params: {'my_xgb__colsample_bytree': 0.6, 'my_xgb__gamma': 0.5, 'my_xgb__max_depth': 3, 'my_xgb__subsample': 0.6, 'tfidf__analyzer': 'char', 'tfidf__max_df': 0.2, 'tfidf__min_df': 5, 'tfidf__ngram_range': (1, 2)}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# trial 4\n",
        "Comparing the obtained ROC AUC score of 0.7281 in Trial 4 with the defined parameters, as well as comparing it with Trials 1, 2, and 3,\n",
        "\n",
        "1. **TfidfVectorizer Parameters:**\n",
        "   - `analyzer`: Both \"word\" and \"char\" analyzers were explored in Trial 4, allowing for analysis at both the word and character levels.\n",
        "   - `ngram_range=(1, 2)`: Similar to Trials 1, 2, and 3, Trial 4 utilized unigrams and bigrams in the feature representation.\n",
        "   - `max_df`, `min_df`: These parameters were explored within the ranges of 0.2 to 0.8 and 5 to 20, respectively, allowing for different thresholds of document frequency for feature selection.\n",
        "\n",
        "2. **XGBoost Classifier Parameters:**\n",
        "   - Hyperparameters such as `gamma`, `subsample`, `colsample_bytree`, and `max_depth` were further tuned within specific ranges to optimize the XGBoost classifier's performance. Notably, higher values of `gamma` (2 and 5) were explored in Trial 4.\n",
        "\n",
        "**Analysis:**\n",
        "- The ROC AUC score of 0.7281 in Trial 4 represents a slight improvement compared to Trial 1 (0.7235), Trial 3 (0.6418), but slightly lower than Trial 2 (0.7646). This indicates that the model in Trial 4 was relatively effective in distinguishing between fake and genuine news posts.\n",
        "- Exploring both \"word\" and \"char\" analyzers suggests flexibility in feature extraction, allowing the model to capture both word-level and character-level information from the text data.\n",
        "- The choice of hyperparameters for the XGBoost classifier, particularly a higher `gamma` value (5), may have contributed to the improved performance observed in Trial 4.\n",
        "\n",
        "**Comparison with Trials 1, 2, and 3:**\n",
        "- Trial 4 achieved a higher ROC AUC score compared to Trial 1, indicating improved performance.\n",
        "- While Trial 4 performed better than Trial 3, it still lagged behind Trial 2, which had the highest ROC AUC score among the trials.\n",
        "- The differences in parameter settings, including the choice of analyzers and hyperparameters, likely contributed to the variation in performance between the trials.\n",
        "\n"
      ],
      "metadata": {
        "id": "yZRiypJaNCeS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Define the parameter grid for hyperparameter tuning\n",
        "param_xgb = {\n",
        "    \"tfidf__analyzer\":[\"word\",\"char\"],\n",
        "    \"tfidf__ngram_range\": [(1, 2)],\n",
        "    \"tfidf__max_df\": np.arange(0.2, 0.8),\n",
        "    \"tfidf__min_df\": np.arange(5, 20),\n",
        "    'my_xgb__gamma': [ 2, 5],\n",
        "    'my_xgb__subsample': [ 0.8],\n",
        "    'my_xgb__colsample_bytree': [0.8],\n",
        "    'my_xgb__max_depth': [ 4]\n",
        "\n",
        "}\n",
        "\n",
        "# Create GridSearchCV object with the defined parameter grid\n",
        "grid_search2 = GridSearchCV(xgb_pipeline, param_xgb, cv=2, verbose=1, n_jobs=2, scoring='roc_auc')\n",
        "\n",
        "# Fit the GridSearchCV object to the data\n",
        "grid_search2.fit(X_train,y_train)\n",
        "\n",
        "# Predict probabilities using the best estimator found by GridSearchCV\n",
        "y_pred_proba = grid_search2.predict(X_val)\n",
        "\n",
        "# Print the ROC AUC score using the true labels and predicted probabilities\n",
        "print(roc_auc_score(y_val, y_pred_proba))\n",
        "\n",
        "# Print the best parameters found by GridSearchCV\n",
        "print('best params: {}'.format(grid_search2.best_params_))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rFtnevQhA6ty",
        "outputId": "0c326bf9-ae71-4ccc-c6df-663c775cda7d"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 2 folds for each of 60 candidates, totalling 120 fits\n",
            "0.7280775897010723\n",
            "best params: {'my_xgb__colsample_bytree': 0.8, 'my_xgb__gamma': 5, 'my_xgb__max_depth': 4, 'my_xgb__subsample': 0.8, 'tfidf__analyzer': 'word', 'tfidf__max_df': 0.2, 'tfidf__min_df': 5, 'tfidf__ngram_range': (1, 2)}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# trial 5\n",
        "\n",
        "Comparing the obtained ROC AUC score of 0.7670 in Trial 5 with the defined parameters, as well as comparing it with Trials 1, 2, 3, and 4,\n",
        "\n",
        "1. **TfidfVectorizer Parameters:**\n",
        "   - `analyzer`: Both \"word\" and \"char\" analyzers were explored in Trial 5, allowing for analysis at both the word and character levels.\n",
        "   - `ngram_range=(1, 3)`: Unlike previous trials, Trial 5 extended the ngram range to include trigrams in addition to unigrams and bigrams. This allows the model to capture more complex patterns in the text data.\n",
        "   - `max_df`, `min_df`: These parameters were explored within the ranges of 0.2 to 0.8 and 5 to 20, respectively, allowing for different thresholds of document frequency for feature selection.\n",
        "\n",
        "**Analysis:**\n",
        "- The ROC AUC score of 0.7670 in Trial 5 represents an improvement compared to Trials 1, 2, 3, and 4, indicating that the model in Trial 5 was more effective in distinguishing between fake and genuine news posts.\n",
        "- Exploring both \"word\" and \"char\" analyzers and extending the ngram range to include trigrams allowed the model to capture more nuanced information from the text data, leading to improved performance.\n",
        "- The choice of hyperparameters for the TfidfVectorizer appears to have been effective, as the selected parameters led to better performance compared to previous trials.\n",
        "\n",
        "**Comparison with Previous Trials:**\n",
        "- Trial 5 achieved the highest ROC AUC score among all trials conducted so far, indicating that it is the best-performing model.\n",
        "- The inclusion of trigrams in the feature representation, along with the exploration of both word and character analyzers, likely contributed to the improved performance observed in Trial 5.\n",
        "- While previous trials explored various parameter settings and preprocessing techniques, Trial 5's combination of parameters led to the most successful outcome.\n",
        "\n",
        "**Best Parameters:**\n",
        "- The best parameters found by GridSearchCV in Trial 5 were:\n",
        "  - TfidfVectorizer analyzer: 'word'\n",
        "  - TfidfVectorizer max_df: 0.2\n",
        "  - TfidfVectorizer min_df: 5\n",
        "  - TfidfVectorizer ngram_range: (1, 3)\n",
        "\n",
        "**Conclusion:**\n",
        "- Trial 5 represents the best-performing model so far, achieving the highest ROC AUC score among all trials conducted.\n",
        "- The combination of extending the ngram range to include trigrams and exploring both word and character analyzers proved to be effective in capturing important features from the text data.\n",
        "- Moving forward, further refinements and optimizations may still be explored to potentially improve model performance even further."
      ],
      "metadata": {
        "id": "cA6-Z9uUNEwk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Define the parameter grid for hyperparameter tuning\n",
        "param_xgb = {\n",
        "       \"tfidf__analyzer\":[\"word\",\"char\"],\n",
        "       \"tfidf__ngram_range\": [(1, 3)],\n",
        "    \"tfidf__max_df\": np.arange(0.2, 0.8),\n",
        "    \"tfidf__min_df\": np.arange(5, 20)\n",
        "}\n",
        "\n",
        "# Create GridSearchCV object with the defined parameter grid\n",
        "grid_search3 = GridSearchCV(RF_pipeline, param_xgb, cv=2, verbose=1, n_jobs=2, scoring='roc_auc')\n",
        "\n",
        "# Fit the GridSearchCV object to the data\n",
        "grid_search3.fit(X_train,y_train)\n",
        "\n",
        "# Predict probabilities using the best estimator found by GridSearchCV\n",
        "y_pred_proba = grid_search3.predict(X_val)\n",
        "\n",
        "# Print the ROC AUC score using the true labels and predicted probabilities\n",
        "print(roc_auc_score(y_val, y_pred_proba))\n",
        "\n",
        "# Print the best parameters found by GridSearchCV\n",
        "print('best params: {}'.format(grid_search3.best_params_))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rlzacYSHCE5C",
        "outputId": "38ab9b34-a8fb-4b11-99cc-904ec70832de"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 2 folds for each of 30 candidates, totalling 60 fits\n",
            "0.7670219414375964\n",
            "best params: {'tfidf__analyzer': 'word', 'tfidf__max_df': 0.2, 'tfidf__min_df': 5, 'tfidf__ngram_range': (1, 3)}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# testing data"
      ],
      "metadata": {
        "id": "DppdlXypNJAJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dftest = pd.read_csv(\"x_test.csv\")\n",
        "dftest[\"text\"] = dftest[\"text\"].apply(cleantext)\n",
        "print(dftest.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qhs_zTw4Dxxb",
        "outputId": "8b7c331e-f9a2-4c46-8f55-4638da07c4db"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   id                                               text\n",
            "0   0                                          stargazer\n",
            "1   1                                               yeah\n",
            "2   2  pd phoenix car thief get instruction youtube v...\n",
            "3   3         trump accuses iran one problem credibility\n",
            "4   4                                 believer hezbollah\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(dftest.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e2XwqbclKT94",
        "outputId": "486d37f7-f81e-4a35-b373-3a44e461d5cf"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(59151, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "\n",
        "y_pred_proba = xgb_pipeline.predict_proba(dftest['text'])\n",
        "#res=metrics.roc_auc_score(y_val, y_pred_proba)\n",
        "print(y_pred_proba)\n",
        "#print(dftest)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v_YhIwW2mdqH",
        "outputId": "383326c7-78e6-4c55-cf42-fc8476c7720d"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.51305467 0.48694533]\n",
            " [0.51305467 0.48694533]\n",
            " [0.44671005 0.55328995]\n",
            " ...\n",
            " [0.94100595 0.05899406]\n",
            " [0.51305467 0.48694533]\n",
            " [0.41156864 0.58843136]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Define a function to save predicted probabilities to CSV with 'id' and 'label' headers\n",
        "def save_proba_to_csv_with_headers(dftest, model, filename):\n",
        "    # Make predictions using the provided model object\n",
        "    y_pred_proba = model.predict_proba(dftest['text'])\n",
        "    #print(y_pred_proba)\n",
        "    # Create a DataFrame with 'id' and 'label' columns\n",
        "    df_output = pd.DataFrame({'id': dftest['id'], 'label': y_pred_proba[:,1]})\n",
        "    # Save the DataFrame to CSV file without index and with header\n",
        "    df_output.to_csv(filename, index=False)\n",
        "\n",
        "# Example usage:\n",
        "# Save predicted probabilities from grid_search1 with 'id' and 'label' headers\n",
        "save_proba_to_csv_with_headers(dftest, grid_search1, 'grid_search1_proba.csv')\n",
        "\n",
        "# Save predicted probabilities from grid_search2 with 'id' and 'label' headers\n",
        "save_proba_to_csv_with_headers(dftest, grid_search2, 'grid_search2_proba.csv')\n",
        "\n",
        "# Save predicted probabilities from grid_search3 with 'id' and 'label' headers\n",
        "save_proba_to_csv_with_headers(dftest, grid_search3, 'grid_search3_proba.csv')\n",
        "\n",
        "# Save predicted probabilities from xgb_pipline with 'id' and 'label' headers\n",
        "save_proba_to_csv_with_headers(dftest, xgb_pipeline, 'xgb_pipline_proba.csv')\n",
        "\n",
        "# Save predicted probabilities from RF_pipline with 'id' and 'label' headers\n",
        "save_proba_to_csv_with_headers(dftest, RF_pipeline, 'RF_pipline_proba.csv')\n"
      ],
      "metadata": {
        "id": "dO12fMAtDYsK"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "s97G2IiLFxpn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}